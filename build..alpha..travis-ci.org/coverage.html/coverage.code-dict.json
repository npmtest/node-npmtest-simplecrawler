{"/home/travis/build/npmtest/node-npmtest-simplecrawler/test.js":"/* istanbul instrument in package npmtest_simplecrawler */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        switch (local.modeJs) {\n        // re-init local from window.local\n        case 'browser':\n            local = local.global.utility2.objectSetDefault(\n                local.global.utility2_rollup || local.global.local,\n                local.global.utility2\n            );\n            break;\n        // re-init local from example.js\n        case 'node':\n            local = (local.global.utility2_rollup || require('utility2'))\n                .requireExampleJsFromReadme();\n            break;\n        }\n        // export local\n        local.global.local = local;\n    }());\n\n\n\n    // run shared js-env code - function\n    (function () {\n        return;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // run browser js-env code - function\n    case 'browser':\n        break;\n\n\n\n    // run node js-env code - function\n    case 'node':\n        break;\n    }\n\n\n\n    // run shared js-env code - post-init\n    (function () {\n        return;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // run browser js-env code - post-init\n    case 'browser':\n        local.testCase_browser_nullCase = local.testCase_browser_nullCase || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test browsers's null-case handling-behavior-behavior\n         */\n            onError(null, options);\n        };\n\n        // run tests\n        local.nop(local.modeTest &&\n            document.querySelector('#testRunButton1') &&\n            document.querySelector('#testRunButton1').click());\n        break;\n\n\n\n    // run node js-env code - post-init\n    /* istanbul ignore next */\n    case 'node':\n        local.testCase_buildApidoc_default = local.testCase_buildApidoc_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildApidoc's default handling-behavior-behavior\n         */\n            options = { modulePathList: module.paths };\n            local.buildApidoc(options, onError);\n        };\n\n        local.testCase_buildApp_default = local.testCase_buildApp_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildApp's default handling-behavior-behavior\n         */\n            local.testCase_buildReadme_default(options, local.onErrorThrow);\n            local.testCase_buildLib_default(options, local.onErrorThrow);\n            local.testCase_buildTest_default(options, local.onErrorThrow);\n            local.testCase_buildCustomOrg_default(options, local.onErrorThrow);\n            options = [];\n            local.buildApp(options, onError);\n        };\n\n        local.testCase_buildCustomOrg_default = local.testCase_buildCustomOrg_default ||\n            function (options, onError) {\n            /*\n             * this function will test buildCustomOrg's default handling-behavior\n             */\n                options = {};\n                local.buildCustomOrg(options, onError);\n            };\n\n        local.testCase_buildLib_default = local.testCase_buildLib_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildLib's default handling-behavior\n         */\n            options = {};\n            local.buildLib(options, onError);\n        };\n\n        local.testCase_buildReadme_default = local.testCase_buildReadme_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildReadme's default handling-behavior-behavior\n         */\n            options = {};\n            local.buildReadme(options, onError);\n        };\n\n        local.testCase_buildTest_default = local.testCase_buildTest_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildTest's default handling-behavior\n         */\n            options = {};\n            local.buildTest(options, onError);\n        };\n\n        local.testCase_webpage_default = local.testCase_webpage_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test webpage's default handling-behavior\n         */\n            options = { modeCoverageMerge: true, url: local.serverLocalHost + '?modeTest=1' };\n            local.browserTest(options, onError);\n        };\n\n        // run test-server\n        local.testRunServer(local);\n        break;\n    }\n}());\n","/home/travis/build/npmtest/node-npmtest-simplecrawler/lib.npmtest_simplecrawler.js":"/* istanbul instrument in package npmtest_simplecrawler */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        // init utility2_rollup\n        local = local.global.utility2_rollup || local;\n        // init lib\n        local.local = local.npmtest_simplecrawler = local;\n        // init exports\n        if (local.modeJs === 'browser') {\n            local.global.utility2_npmtest_simplecrawler = local;\n        } else {\n            module.exports = local;\n            module.exports.__dirname = __dirname;\n            module.exports.module = module;\n        }\n    }());\n}());\n","/home/travis/build/npmtest/node-npmtest-simplecrawler/example.js":"/*\nexample.js\n\nquickstart example\n\ninstruction\n    1. save this script as example.js\n    2. run the shell command:\n        $ npm install npmtest-simplecrawler && PORT=8081 node example.js\n    3. play with the browser-demo on http://127.0.0.1:8081\n*/\n\n\n\n/* istanbul instrument in package npmtest_simplecrawler */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - pre-init\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        // init utility2_rollup\n        local = local.global.utility2_rollup || (local.modeJs === 'browser'\n            ? local.global.utility2_npmtest_simplecrawler\n            : global.utility2_moduleExports);\n        // export local\n        local.global.local = local;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // post-init\n    // run browser js-env code - post-init\n    /* istanbul ignore next */\n    case 'browser':\n        local.testRunBrowser = function (event) {\n            if (!event || (event &&\n                    event.currentTarget &&\n                    event.currentTarget.className &&\n                    event.currentTarget.className.includes &&\n                    event.currentTarget.className.includes('onreset'))) {\n                // reset output\n                Array.from(\n                    document.querySelectorAll('body > .resettable')\n                ).forEach(function (element) {\n                    switch (element.tagName) {\n                    case 'INPUT':\n                    case 'TEXTAREA':\n                        element.value = '';\n                        break;\n                    default:\n                        element.textContent = '';\n                    }\n                });\n            }\n            switch (event && event.currentTarget && event.currentTarget.id) {\n            case 'testRunButton1':\n                // show tests\n                if (document.querySelector('#testReportDiv1').style.display === 'none') {\n                    document.querySelector('#testReportDiv1').style.display = 'block';\n                    document.querySelector('#testRunButton1').textContent =\n                        'hide internal test';\n                    local.modeTest = true;\n                    local.testRunDefault(local);\n                // hide tests\n                } else {\n                    document.querySelector('#testReportDiv1').style.display = 'none';\n                    document.querySelector('#testRunButton1').textContent = 'run internal test';\n                }\n                break;\n            // custom-case\n            default:\n                break;\n            }\n            if (document.querySelector('#inputTextareaEval1') && (!event || (event &&\n                    event.currentTarget &&\n                    event.currentTarget.className &&\n                    event.currentTarget.className.includes &&\n                    event.currentTarget.className.includes('oneval')))) {\n                // try to eval input-code\n                try {\n                    /*jslint evil: true*/\n                    eval(document.querySelector('#inputTextareaEval1').value);\n                } catch (errorCaught) {\n                    console.error(errorCaught);\n                }\n            }\n        };\n        // log stderr and stdout to #outputTextareaStdout1\n        ['error', 'log'].forEach(function (key) {\n            console[key + '_original'] = console[key];\n            console[key] = function () {\n                var element;\n                console[key + '_original'].apply(console, arguments);\n                element = document.querySelector('#outputTextareaStdout1');\n                if (!element) {\n                    return;\n                }\n                // append text to #outputTextareaStdout1\n                element.value += Array.from(arguments).map(function (arg) {\n                    return typeof arg === 'string'\n                        ? arg\n                        : JSON.stringify(arg, null, 4);\n                }).join(' ') + '\\n';\n                // scroll textarea to bottom\n                element.scrollTop = element.scrollHeight;\n            };\n        });\n        // init event-handling\n        ['change', 'click', 'keyup'].forEach(function (event) {\n            Array.from(document.querySelectorAll('.on' + event)).forEach(function (element) {\n                element.addEventListener(event, local.testRunBrowser);\n            });\n        });\n        // run tests\n        local.testRunBrowser();\n        break;\n\n\n\n    // run node js-env code - post-init\n    /* istanbul ignore next */\n    case 'node':\n        // export local\n        module.exports = local;\n        // require modules\n        local.fs = require('fs');\n        local.http = require('http');\n        local.url = require('url');\n        // init assets\n        local.assetsDict = local.assetsDict || {};\n        /* jslint-ignore-begin */\n        local.assetsDict['/assets.index.template.html'] = '\\\n<!doctype html>\\n\\\n<html lang=\"en\">\\n\\\n<head>\\n\\\n<meta charset=\"UTF-8\">\\n\\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\\n<title>{{env.npm_package_name}} (v{{env.npm_package_version}})</title>\\n\\\n<style>\\n\\\n/*csslint\\n\\\n    box-sizing: false,\\n\\\n    universal-selector: false\\n\\\n*/\\n\\\n* {\\n\\\n    box-sizing: border-box;\\n\\\n}\\n\\\nbody {\\n\\\n    background: #dde;\\n\\\n    font-family: Arial, Helvetica, sans-serif;\\n\\\n    margin: 2rem;\\n\\\n}\\n\\\nbody > * {\\n\\\n    margin-bottom: 1rem;\\n\\\n}\\n\\\n.utility2FooterDiv {\\n\\\n    margin-top: 20px;\\n\\\n    text-align: center;\\n\\\n}\\n\\\n</style>\\n\\\n<style>\\n\\\n/*csslint\\n\\\n*/\\n\\\ntextarea {\\n\\\n    font-family: monospace;\\n\\\n    height: 10rem;\\n\\\n    width: 100%;\\n\\\n}\\n\\\ntextarea[readonly] {\\n\\\n    background: #ddd;\\n\\\n}\\n\\\n</style>\\n\\\n</head>\\n\\\n<body>\\n\\\n<!-- utility2-comment\\n\\\n<div id=\"ajaxProgressDiv1\" style=\"background: #d00; height: 2px; left: 0; margin: 0; padding: 0; position: fixed; top: 0; transition: background 0.5s, width 1.5s; width: 25%;\"></div>\\n\\\nutility2-comment -->\\n\\\n<h1>\\n\\\n<!-- utility2-comment\\n\\\n    <a\\n\\\n        {{#if env.npm_package_homepage}}\\n\\\n        href=\"{{env.npm_package_homepage}}\"\\n\\\n        {{/if env.npm_package_homepage}}\\n\\\n        target=\"_blank\"\\n\\\n    >\\n\\\nutility2-comment -->\\n\\\n        {{env.npm_package_name}} (v{{env.npm_package_version}})\\n\\\n<!-- utility2-comment\\n\\\n    </a>\\n\\\nutility2-comment -->\\n\\\n</h1>\\n\\\n<h3>{{env.npm_package_description}}</h3>\\n\\\n<!-- utility2-comment\\n\\\n<h4><a download href=\"assets.app.js\">download standalone app</a></h4>\\n\\\n<button class=\"onclick onreset\" id=\"testRunButton1\">run internal test</button><br>\\n\\\n<div id=\"testReportDiv1\" style=\"display: none;\"></div>\\n\\\nutility2-comment -->\\n\\\n\\n\\\n\\n\\\n\\n\\\n<label>stderr and stdout</label>\\n\\\n<textarea class=\"resettable\" id=\"outputTextareaStdout1\" readonly></textarea>\\n\\\n<!-- utility2-comment\\n\\\n{{#if isRollup}}\\n\\\n<script src=\"assets.app.js\"></script>\\n\\\n{{#unless isRollup}}\\n\\\nutility2-comment -->\\n\\\n<script src=\"assets.utility2.rollup.js\"></script>\\n\\\n<script src=\"jsonp.utility2._stateInit?callback=window.utility2._stateInit\"></script>\\n\\\n<script src=\"assets.npmtest_simplecrawler.rollup.js\"></script>\\n\\\n<script src=\"assets.example.js\"></script>\\n\\\n<script src=\"assets.test.js\"></script>\\n\\\n<!-- utility2-comment\\n\\\n{{/if isRollup}}\\n\\\nutility2-comment -->\\n\\\n<div class=\"utility2FooterDiv\">\\n\\\n    [ this app was created with\\n\\\n    <a href=\"https://github.com/kaizhu256/node-utility2\" target=\"_blank\">utility2</a>\\n\\\n    ]\\n\\\n</div>\\n\\\n</body>\\n\\\n</html>\\n\\\n';\n        /* jslint-ignore-end */\n        if (local.templateRender) {\n            local.assetsDict['/'] = local.templateRender(\n                local.assetsDict['/assets.index.template.html'],\n                {\n                    env: local.objectSetDefault(local.env, {\n                        npm_package_description: 'the greatest app in the world!',\n                        npm_package_name: 'my-app',\n                        npm_package_nameAlias: 'my_app',\n                        npm_package_version: '0.0.1'\n                    })\n                }\n            );\n        } else {\n            local.assetsDict['/'] = local.assetsDict['/assets.index.template.html']\n                .replace((/\\{\\{env\\.(\\w+?)\\}\\}/g), function (match0, match1) {\n                    // jslint-hack\n                    String(match0);\n                    switch (match1) {\n                    case 'npm_package_description':\n                        return 'the greatest app in the world!';\n                    case 'npm_package_name':\n                        return 'my-app';\n                    case 'npm_package_nameAlias':\n                        return 'my_app';\n                    case 'npm_package_version':\n                        return '0.0.1';\n                    }\n                });\n        }\n        // run the cli\n        if (local.global.utility2_rollup || module !== require.main) {\n            break;\n        }\n        local.assetsDict['/assets.example.js'] =\n            local.assetsDict['/assets.example.js'] ||\n            local.fs.readFileSync(__filename, 'utf8');\n        // bug-workaround - long $npm_package_buildCustomOrg\n        /* jslint-ignore-begin */\n        local.assetsDict['/assets.npmtest_simplecrawler.rollup.js'] =\n            local.assetsDict['/assets.npmtest_simplecrawler.rollup.js'] ||\n            local.fs.readFileSync(\n                local.npmtest_simplecrawler.__dirname + '/lib.npmtest_simplecrawler.js',\n                'utf8'\n            ).replace((/^#!/), '//');\n        /* jslint-ignore-end */\n        local.assetsDict['/favicon.ico'] = local.assetsDict['/favicon.ico'] || '';\n        // if $npm_config_timeout_exit exists,\n        // then exit this process after $npm_config_timeout_exit ms\n        if (Number(process.env.npm_config_timeout_exit)) {\n            setTimeout(process.exit, Number(process.env.npm_config_timeout_exit));\n        }\n        // start server\n        if (local.global.utility2_serverHttp1) {\n            break;\n        }\n        process.env.PORT = process.env.PORT || '8081';\n        console.error('server starting on port ' + process.env.PORT);\n        local.http.createServer(function (request, response) {\n            request.urlParsed = local.url.parse(request.url);\n            if (local.assetsDict[request.urlParsed.pathname] !== undefined) {\n                response.end(local.assetsDict[request.urlParsed.pathname]);\n                return;\n            }\n            response.statusCode = 404;\n            response.end();\n        }).listen(process.env.PORT);\n        break;\n    }\n}());\n","/home/travis/build/npmtest/node-npmtest-simplecrawler/node_modules/simplecrawler/lib/index.js":"/*\n * Simplecrawler - Export interfaces\n * https://github.com/cgiffard/node-simplecrawler\n *\n * Copyright (c) 2011-2015, Christopher Giffard\n *\n */\n\nmodule.exports = require(\"./crawler.js\");\n\nmodule.exports.queue = require(\"./queue.js\");\nmodule.exports.cache = require(\"./cache.js\");\n\nmodule.exports.crawl = function () {\n    throw new Error(\n        \"Crawler.crawl is deprecated as of version 1.0.0! \" +\n        \"You can now pass a single URL directly to the constructor. \" +\n        \"See the documentation for more details!\"\n    );\n};\n","/home/travis/build/npmtest/node-npmtest-simplecrawler/node_modules/simplecrawler/lib/crawler.js":"/**\n * @file simplecrawler is a straightforward, event driven web crawler\n * @author Christopher Giffard <christopher.giffard@cgiffard.com>\n * @author Fredrik Ekelund <fredrik@fredrik.computer>\n */\n\nvar FetchQueue  = require(\"./queue.js\"),\n    CookieJar   = require(\"./cookies.js\"),\n    packageJson = require(\"../package.json\");\n\nvar http            = require(\"http\"),\n    https           = require(\"https\"),\n    EventEmitter    = require(\"events\").EventEmitter,\n    uri             = require(\"urijs\"),\n    async           = require(\"async\"),\n    zlib            = require(\"zlib\"),\n    util            = require(\"util\"),\n    iconv           = require(\"iconv-lite\"),\n    robotsTxtParser = require(\"robots-parser\");\n\nvar QUEUE_ITEM_INITIAL_DEPTH = 1;\n\n/**\n * Creates a new crawler\n * @class\n * @param  {String} initialURL The initial URL to fetch. The hostname that the crawler will confine requests to by default is inferred from this URL.\n * @return {Crawler}           Returns the crawler instance to enable chained API calls\n */\nvar Crawler = function(initialURL) {\n    // Allow the crawler to be initialized without the `new` operator. This is\n    // handy for chaining API calls\n    if (!(this instanceof Crawler)) {\n        return new Crawler(initialURL);\n    }\n\n    if (arguments.length > 1) {\n        throw new Error(\"Since 1.0.0, simplecrawler takes a single URL when initialized. Protocol, hostname, port and path are inferred from that argument.\");\n    }\n\n    if (typeof initialURL !== \"string\") {\n        throw new Error(\"The crawler needs a URL string to know where to start crawling\");\n    }\n\n    EventEmitter.call(this);\n\n    var crawler = this,\n        parsedURL = uri(initialURL).normalize();\n\n    /**\n     * Controls which URL to request first\n     * @type {String}\n     */\n    this.initialURL = initialURL;\n\n    /**\n     * Determines what hostname the crawler should limit requests to (so long as\n     * {@link Crawler#filterByDomain} is true)\n     * @type {String}\n     */\n    this.host = parsedURL.hostname();\n\n    /**\n     * Determines the interval at which new requests are spawned by the crawler,\n     * as long as the number of open requests is under the\n     * {@link Crawler#maxConcurrency} cap.\n     * @type {Number}\n     */\n    this.interval = 250;\n\n    /**\n     * Maximum request concurrency. If necessary, simplecrawler will increase\n     * node's http agent maxSockets value to match this setting.\n     * @type {Number}\n     */\n    this.maxConcurrency = 5;\n\n    /**\n     * Maximum time we'll wait for headers\n     * @type {Number}\n     */\n    this.timeout = 300000; // 5 minutes\n\n    /**\n     * Maximum time we'll wait for async listeners\n     * @type {Number}\n     */\n    this.listenerTTL = 10000; // 10 seconds\n\n    /**\n     * Crawler's user agent string\n     * @type {String}\n     * @default \"Node/simplecrawler <version> (https://github.com/cgiffard/node-simplecrawler)\"\n     */\n    this.userAgent =\n        \"Node/\" + packageJson.name + \" \" + packageJson.version +\n        \" (\" + packageJson.repository.url + \")\";\n\n    /**\n     * Queue for requests. The crawler can use any implementation so long as it\n     * uses the same interface. The default queue is simply backed by an array.\n     * @type {FetchQueue}\n     */\n    this.queue = new FetchQueue();\n\n    /**\n     * Controls whether the crawler respects the robots.txt rules of any domain.\n     * This is done both with regards to the robots.txt file, and `<meta>` tags\n     * that specify a `nofollow` value for robots. The latter only applies if\n     * the default {@link Crawler#discoverResources} method is used, though.\n     * @type {Boolean}\n     */\n    this.respectRobotsTxt = true;\n\n    /**\n     * Controls whether the crawler is allowed to change the\n     * {@link Crawler#host} setting if the first response is a redirect to\n     * another domain.\n     * @type {Boolean}\n     */\n    this.allowInitialDomainChange = false;\n\n    /**\n     * Controls whether HTTP responses are automatically decompressed based on\n     * their Content-Encoding header. If true, it will also assign the\n     * appropriate Accept-Encoding header to requests.\n     * @type {Boolean}\n     */\n    this.decompressResponses = true;\n\n    /**\n     * Controls whether HTTP responses are automatically character converted to\n     * standard JavaScript strings using the {@link https://www.npmjs.com/package/iconv-lite|iconv-lite}\n     * module before emitted in the {@link Crawler#event:fetchcomplete} event.\n     * The character encoding is interpreted from the Content-Type header\n     * firstly, and secondly from any `<meta charset=\"xxx\" />` tags.\n     * @type {Boolean}\n     */\n    this.decodeResponses = false;\n\n    /**\n     * Controls whether the crawler fetches only URL's where the hostname\n     * matches {@link Crawler#host}. Unless you want to be crawling the entire\n     * internet, I would recommend leaving this on!\n     * @type {Boolean}\n     */\n    this.filterByDomain = true;\n\n    /**\n     * Controls whether URL's that points to a subdomain of {@link Crawler#host}\n     * should also be fetched.\n     * @type {Boolean}\n     */\n    this.scanSubdomains = false;\n\n    /**\n     * Controls whether to treat the www subdomain as the same domain as\n     * {@link Crawler#host}. So if {@link http://example.com/example} has\n     * already been fetched, {@link http://www.example.com/example} won't be\n     * fetched also.\n     * @type {Boolean}\n     */\n    this.ignoreWWWDomain = true;\n\n    /**\n     * Controls whether to strip the www subdomain entirely from URL's at queue\n     * item construction time.\n     * @type {Boolean}\n     */\n    this.stripWWWDomain = false;\n\n    /**\n     * Internal cache store. Must implement `SimpleCache` interface. You can\n     * save the site to disk using the built in file system cache like this:\n     *\n     * ```js\n     * crawler.cache = new Crawler.cache('pathToCacheDirectory');\n     * ```\n     * @type {SimpleCache}\n     */\n    this.cache = null;\n\n    /**\n     * Controls whether an HTTP proxy should be used for requests\n     * @type {Boolean}\n     */\n    this.useProxy = false;\n\n    /**\n     * If {@link Crawler#useProxy} is true, this setting controls what hostname\n     * to use for the proxy\n     * @type {String}\n     */\n    this.proxyHostname = \"127.0.0.1\";\n\n    /**\n     * If {@link Crawler#useProxy} is true, this setting controls what port to\n     * use for the proxy\n     * @type {Number}\n     */\n    this.proxyPort = 8123;\n\n    /**\n     * If {@link Crawler#useProxy} is true, this setting controls what username\n     * to use for the proxy\n     * @type {String}\n     */\n    this.proxyUser = null;\n\n    /**\n     * If {@link Crawler#useProxy} is true, this setting controls what password\n     * to use for the proxy\n     * @type {String}\n     */\n    this.proxyPass = null;\n\n    /**\n     * Controls whether to use HTTP Basic Auth\n     * @type {Boolean}\n     */\n    this.needsAuth = false;\n\n    /**\n     * If {@link Crawler#needsAuth} is true, this setting controls what username\n     * to send with HTTP Basic Auth\n     * @type {String}\n     */\n    this.authUser = null;\n\n    /**\n     * If {@link Crawler#needsAuth} is true, this setting controls what password\n     * to send with HTTP Basic Auth\n     * @type {String}\n     */\n    this.authPass = null;\n\n    /**\n     * Controls whether to save and send cookies or not\n     * @type {Boolean}\n     */\n    this.acceptCookies = true;\n\n    /**\n     * The module used to store cookies\n     * @type {CookieJar}\n     */\n    this.cookies = new CookieJar();\n\n    /**\n     * Controls what headers (besides the default ones) to include with every\n     * request.\n     * @type {Object}\n     */\n    this.customHeaders = {};\n\n    /**\n     * Controls what domains the crawler is allowed to fetch from, regardless of\n     * {@link Crawler#host} or {@link Crawler#filterByDomain} settings.\n     * @type {Array}\n     */\n    this.domainWhitelist = [];\n\n    /**\n     * Controls what protocols the crawler is allowed to fetch from\n     * @type {RegExp[]}\n     */\n    this.allowedProtocols = [\n        /^http(s)?$/i,                  // HTTP & HTTPS\n        /^(rss|atom|feed)(\\+xml)?$/i    // RSS / XML\n    ];\n\n    /**\n     * Controls the maximum allowed size in bytes of resources to be fetched\n     * @default 16777216\n     * @type {Number}\n     */\n    this.maxResourceSize = 1024 * 1024 * 16; // 16mb\n\n    /**\n     * Controls what mimetypes the crawler will scan for new resources. If\n     * {@link Crawler#downloadUnsupported} is false, this setting will also\n     * restrict what resources are downloaded.\n     * @type {Array.<RegExp|string>}\n     */\n    this.supportedMimeTypes = [\n        /^text\\//i,\n        /^application\\/(rss|html|xhtml)?[\\+\\/\\-]?xml/i,\n        /^application\\/javascript/i,\n        /^xml/i\n    ];\n\n    /**\n     * Controls whether to download resources with unsupported mimetypes (as\n     * specified by {@link Crawler#supportedMimeTypes})\n     * @type {Boolean}\n     */\n    this.downloadUnsupported = true;\n\n    /**\n     * Controls what URL encoding to use. Can be either \"unicode\" or \"iso8859\"\n     * @type {String}\n     */\n    this.urlEncoding = \"unicode\";\n\n    /**\n     * Controls whether to strip query string parameters from URL's at queue\n     * item construction time.\n     * @type {Boolean}\n     */\n    this.stripQuerystring = false;\n\n    /**\n     * Collection of regular expressions and functions that are applied in the\n     * default {@link Crawler#discoverResources} method.\n     * @type {Array.<RegExp|Function>}\n     */\n    this.discoverRegex = [\n        /\\s(?:href|src)\\s?=\\s?([\"']).*?\\1/ig,\n        /\\s(?:href|src)\\s?=\\s?[^\"'\\s][^\\s>]+/ig,\n        /\\s?url\\(([\"']).*?\\1\\)/ig,\n        /\\s?url\\([^\"'].*?\\)/ig,\n\n        // This could easily duplicate matches above, e.g. in the case of\n        // href=\"http://example.com\"\n        /https?:\\/\\/[^?\\s><'\"]+/ig,\n\n        // This might be a bit of a gamble... but get hard-coded\n        // strings out of javacript: URLs. They're often popup-image\n        // or preview windows, which would otherwise be unavailable to us.\n        // Worst case scenario is we make some junky requests.\n        /^javascript:\\s*[\\w$.]+\\(['\"][^'\"\\s]+/ig,\n\n        // Find srcset links\n        function(string) {\n            var result = /\\ssrcset\\s*=\\s*([\"'])(.*)\\1/.exec(string);\n            return Array.isArray(result) ? String(result[2]).split(\",\").map(function(string) {\n                return string.trim().split(/\\s+/)[0];\n            }) : \"\";\n        },\n\n        // Find resources in <meta> redirects. We need to wrap these RegExp's in\n        // functions because we only want to return the first capture group, not\n        // the entire match. And we need two RegExp's because the necessary\n        // attributes on the <meta> tag can appear in any order\n        function(string) {\n            var match = string.match(/<meta[^>]*http-equiv\\s*=\\s*[\"']?refresh[\"']?[^>]*content\\s*=\\s*[\"'] ?[^\"'>]*url=([^\"'>]*)[\"']?[^>]*>/i);\n            return Array.isArray(match) ? [match[1]] : undefined;\n        },\n        function(string) {\n            var match = string.match(/<meta[^>]*content\\s*=\\s*[\"']?[^\"'>]*url=([^\"'>]*)[\"']?[^>]*http-equiv\\s*=\\s*[\"']?refresh[\"']?[^>]*>/i);\n            return Array.isArray(match) ? [match[1]] : undefined;\n        }\n    ];\n\n    /**\n     * Controls whether the default {@link Crawler#discoverResources} should\n     * scan for new resources inside of HTML comments.\n     * @type {Boolean}\n     */\n    this.parseHTMLComments = true;\n\n    /**\n     * Controls whether the default {@link Crawler#discoverResources} should\n     * scan for new resources inside of `<script>` tags.\n     * @type {Boolean}\n     */\n    this.parseScriptTags = true;\n\n    /**\n     * Controls the max depth of resources that the crawler fetches. 0 means\n     * that the crawler won't restrict requests based on depth. The initial\n     * resource, as well as manually queued resources, are at depth 1. From\n     * there, every discovered resource adds 1 to its referrer's depth.\n     * @type {Number}\n     */\n    this.maxDepth = 0;\n\n    /**\n     * Controls whether to proceed anyway when the crawler encounters an invalid\n     * SSL certificate.\n     * @type {Boolean}\n     */\n    this.ignoreInvalidSSL = false;\n\n    /**\n     * Controls what HTTP agent to use. This is useful if you want to configure\n     * eg. a SOCKS client.\n     * @type {HTTPAgent}\n     */\n    this.httpAgent = http.globalAgent;\n\n    /**\n     * Controls what HTTPS agent to use. This is useful if you want to configure\n     * eg. a SOCKS client.\n     * @type {HTTPAgent}\n     */\n    this.httpsAgent = https.globalAgent;\n\n    // STATE (AND OTHER) VARIABLES NOT TO STUFF WITH\n    var hiddenProps = {\n        _downloadConditions: [],\n        _fetchConditions: [],\n        _isFirstRequest: true,\n        _openListeners: 0,\n        _openRequests: [],\n        _robotsTxts: [],\n        _touchedHosts: []\n    };\n\n    // Apply all the hidden props\n    Object.keys(hiddenProps).forEach(function(key) {\n        Object.defineProperty(crawler, key, {\n            writable: true,\n            enumerable: false,\n            value: hiddenProps[key]\n        });\n    });\n};\n\nutil.inherits(Crawler, EventEmitter);\n\n/**\n * Starts or resumes the crawl. It adds a queue item constructed from\n * {@link Crawler#initialURL} to the queue. The crawler waits for\n * process.nextTick to begin, so handlers and other properties can be altered or\n * addressed before the crawl commences.\n * @return {Crawler} Returns the crawler instance to enable chained API calls\n */\nCrawler.prototype.start = function() {\n    var crawler = this;\n\n    if (crawler.running) {\n        return crawler;\n    }\n\n    crawler.running = true;\n\n    var queueItem = crawler.processURL(crawler.initialURL);\n    queueItem.referrer = undefined;\n    queueItem.depth = QUEUE_ITEM_INITIAL_DEPTH;\n\n    crawler.queue.add(queueItem, false, function(error) {\n        if (error && error.code !== \"DUPLICATE\") {\n            throw error;\n        }\n\n        process.nextTick(function() {\n            crawler.crawlIntervalID = setInterval(crawler.crawl.bind(crawler),\n                crawler.interval);\n\n            crawler.crawl();\n        });\n\n        /**\n         * Fired when the crawl starts. This event gives you the opportunity to\n         * adjust the crawler's configuration, since the crawl won't actually start\n         * until the next processor tick.\n         * @event Crawler#fetchstart\n         */\n        crawler.emit(\"crawlstart\");\n    });\n\n    return crawler;\n};\n\n/**\n * Determines whether robots.txt rules allows the fetching of a particular URL\n * or not\n * @param  {String} url The full URL of the resource that is to be fetched (or not)\n * @return {Boolean}    Returns true if the URL is allowed to be fetched, otherwise false\n */\nCrawler.prototype.urlIsAllowed = function(url) {\n    var crawler = this;\n\n    var formattedURL = uri(url).normalize().href(),\n        allowed = false;\n\n    // The punycode module sometimes chokes on really weird domain\n    // names. Catching those errors to prevent crawler from crashing\n    try {\n        allowed = crawler._robotsTxts.reduce(function(result, robots) {\n            var allowed = robots.isAllowed(formattedURL, crawler.userAgent);\n            return result !== undefined ? result : allowed;\n        }, undefined);\n    } catch (error) {\n        // URL will be avoided\n    }\n\n    return allowed === undefined ? true : allowed;\n};\n\n/**\n * Generates a configuration object for http[s].request\n * @param  {QueueItem} queueItem The queue item for which a request option object should be generated\n * @return {Object}              Returns an object that can be passed directly to http[s].request\n */\nCrawler.prototype.getRequestOptions = function(queueItem) {\n    var crawler = this;\n\n    var agent = queueItem.protocol === \"https\" ? crawler.httpsAgent : crawler.httpAgent;\n\n    // Extract request options from queue;\n    var requestHost = queueItem.host,\n        requestPort = queueItem.port,\n        requestPath = queueItem.path;\n\n    // Are we passing through an HTTP proxy?\n    if (crawler.useProxy) {\n        requestHost = crawler.proxyHostname;\n        requestPort = crawler.proxyPort;\n        requestPath = queueItem.url;\n    }\n\n    var isStandardHTTPPort = queueItem.protocol === \"http\" && queueItem.port !== 80,\n        isStandardHTTPSPort = queueItem.protocol === \"https\" && queueItem.port !== 443,\n        isStandardPort = isStandardHTTPPort || isStandardHTTPSPort;\n\n    // Load in request options\n    var requestOptions = {\n        method: \"GET\",\n        host: requestHost,\n        port: requestPort,\n        path: requestPath,\n        agent: agent,\n        headers: {\n            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n            \"User-Agent\": crawler.userAgent,\n            \"Host\": queueItem.host + (queueItem.port && isStandardPort ? \":\" + queueItem.port : \"\")\n        }\n    };\n\n    if (crawler.decompressResponses) {\n        requestOptions.headers[\"Accept-Encoding\"] = \"gzip, deflate\";\n    }\n\n    if (queueItem.referrer) {\n        requestOptions.headers.Referer = queueItem.referrer;\n    }\n\n    // If port is one of the HTTP/HTTPS defaults, delete the option to avoid conflicts\n    if (requestPort === 80 || requestPort === 443 || !requestPort) {\n        delete requestOptions.port;\n    }\n\n    // Add cookie header from cookie jar if we're configured to\n    // send/accept cookies\n    if (crawler.acceptCookies && crawler.cookies.getAsHeader()) {\n        requestOptions.headers.cookie =\n            crawler.cookies.getAsHeader(queueItem.host, queueItem.path);\n    }\n\n    // Add auth headers if we need them\n    if (crawler.needsAuth) {\n        var auth = crawler.authUser + \":\" + crawler.authPass;\n\n        // Generate auth header\n        auth = \"Basic \" + new Buffer(auth).toString(\"base64\");\n        requestOptions.headers.Authorization = auth;\n    }\n\n    // Add proxy auth if we need it\n    if (crawler.proxyUser !== null && crawler.proxyPass !== null) {\n        var proxyAuth = crawler.proxyUser + \":\" + crawler.proxyPass;\n\n        // Generate auth header\n        proxyAuth = \"Basic \" + new Buffer(proxyAuth).toString(\"base64\");\n        requestOptions.headers[\"Proxy-Authorization\"] = proxyAuth;\n    }\n\n    // And if we've got any custom headers available\n    if (crawler.customHeaders) {\n        for (var header in crawler.customHeaders) {\n            if (crawler.customHeaders.hasOwnProperty(header)) {\n                requestOptions.headers[header] = crawler.customHeaders[header];\n            }\n        }\n    }\n\n    return requestOptions;\n};\n\n/**\n * Performs an HTTP request for the robots.txt resource on any domain\n * @param  {String} url                            The full URL to the robots.txt file, eg. \"http://example.com/robots.txt\"\n * @param  {Crawler~getRobotsTxtCallback} callback The callback called with the server's response, or an error\n * @return {Crawler}                               Returns the crawler instance to enable chained API calls\n */\nCrawler.prototype.getRobotsTxt = function(url, callback) {\n    var crawler = this,\n        errorMsg;\n\n    var robotsTxtUrl = uri(url);\n    var client = robotsTxtUrl.protocol() === \"https\" ? https : http;\n\n    // Apply the ignoreInvalidSSL setting to https connections\n    if (client === https && crawler.ignoreInvalidSSL === true) {\n        client.rejectUnauthorized = false;\n        client.strictSSL = false;\n    }\n\n    var requestOptions = crawler.getRequestOptions(crawler.processURL(robotsTxtUrl.href()));\n\n    // Get the resource!\n    var clientRequest = client.request(requestOptions, function(response) {\n        if (response.statusCode >= 200 && response.statusCode < 300) {\n            var responseLength =\n                    parseInt(response.headers[\"content-length\"], 10) ||\n                    crawler.maxResourceSize,\n                responseBuffer = new Buffer(responseLength),\n                responseLengthReceived = 0;\n\n            response.on(\"data\", function(chunk) {\n                if (responseLengthReceived + chunk.length <= crawler.maxResourceSize) {\n                    chunk.copy(responseBuffer, responseLengthReceived, 0, chunk.length);\n                    responseLengthReceived += chunk.length;\n                } else {\n                    response.destroy();\n                    callback(new Error(\"robots.txt exceeded maxResourceSize\"));\n                }\n            });\n\n            var decodeAndReturnResponse = function(error, responseBuffer) {\n                if (error) {\n                    return callback(new Error(\"Couldn't unzip robots.txt response body\"));\n                }\n\n                var contentType = response.headers[\"content-type\"],\n                    responseBody = crawler.decodeBuffer(responseBuffer, contentType);\n\n                callback(undefined, robotsTxtUrl.href(), responseBody);\n            };\n\n            response.on(\"end\", function() {\n                var contentEncoding = response.headers[\"content-encoding\"];\n\n                if (contentEncoding && /(gzip|deflate)/.test(contentEncoding)) {\n                    zlib.unzip(responseBuffer, decodeAndReturnResponse);\n                } else {\n                    decodeAndReturnResponse(undefined, responseBuffer);\n                }\n            });\n        } else if (response.statusCode >= 300 && response.statusCode < 400 &&\n            response.headers.location) {\n\n            response.destroy();\n\n            var redirectTarget = uri(response.headers.location)\n                .absoluteTo(robotsTxtUrl)\n                .normalize();\n\n            if (crawler.domainValid(redirectTarget.hostname())) {\n                crawler.getRobotsTxt(redirectTarget.href(), callback);\n            } else {\n                errorMsg = util.format(\"%s redirected to a disallowed domain (%s)\", robotsTxtUrl.href(), redirectTarget.hostname());\n                callback(new Error(errorMsg));\n            }\n        } else {\n            response.destroy();\n\n            errorMsg = util.format(\"Server responded with status %d when fetching robots.txt\", response.statusCode);\n            callback(new Error(errorMsg));\n        }\n    });\n\n    clientRequest.end();\n\n    clientRequest.setTimeout(crawler.timeout, function() {\n        clientRequest.abort();\n        callback(new Error(\"robots.txt request timed out\"));\n    });\n\n    clientRequest.on(\"error\", function(errorData) {\n        if (!clientRequest.aborted) {\n            callback(errorData);\n        }\n    });\n\n    return crawler;\n};\n\n/**\n * Determines whether the crawler supports a protocol\n * @param  {String} URL A full URL, eg. \"http://example.com\"\n * @return {Boolean}    Returns true if the protocol of the URL is supported, false if not\n */\nCrawler.prototype.protocolSupported = function(URL) {\n    var protocol,\n        crawler = this;\n\n    try {\n        protocol = uri(URL).protocol();\n\n        // Unspecified protocol. Assume http\n        if (!protocol) {\n            protocol = \"http\";\n        }\n\n    } catch (e) {\n        // If URIjs died, we definitely /do not/ support the protocol.\n        return false;\n    }\n\n    return crawler.allowedProtocols.some(function(protocolCheck) {\n        return protocolCheck.test(protocol);\n    });\n};\n\n/**\n * Determines whether the crawler supports a mimetype\n * @param  {String} mimetype Eg. \"text/html\" or \"application/octet-stream\"\n * @return {Boolean}         Returns true if the mimetype is supported, false if not\n */\nCrawler.prototype.mimeTypeSupported = function(mimetype) {\n    var crawler = this;\n\n    return crawler.supportedMimeTypes.some(function(mimeCheck) {\n        if (typeof mimeCheck === \"string\") {\n            return mimeCheck === mimetype;\n        }\n\n        return mimeCheck.test(mimetype);\n    });\n};\n\n/**\n * Constructs a queue item from a URL and a referrer queue item.\n * @param  {String} url           An absolute or relative URL to construct a queue item from\n * @param  {QueueItem} [referrer] The queue item representing the resource where this URL was discovered\n * @return {QueueItem}            Returns a new queue item\n */\nCrawler.prototype.processURL = function(url, referrer) {\n    var newUrl,\n        crawler = this;\n\n    if (typeof referrer !== \"object\") {\n        referrer = {\n            url: crawler.initialURL,\n            depth: QUEUE_ITEM_INITIAL_DEPTH - 1\n        };\n    }\n\n    // If the URL didn't contain anything, don't fetch it.\n    if (!(url && url.trim().length)) {\n        return false;\n    }\n\n    // Check if querystring should be ignored\n    if (crawler.stripQuerystring === true) {\n        url = uri(url).search(\"\").href();\n    }\n\n    if (crawler.stripWWWDomain && url.match(/https?:\\/\\/(www\\.).*/i)) {\n        url = url.replace(\"www.\", \"\");\n    }\n\n    try {\n        newUrl = uri(url).absoluteTo(referrer.url).normalize();\n\n        if (crawler.urlEncoding === \"iso8859\") {\n            newUrl = newUrl.iso8859();\n        }\n    } catch (e) {\n        // Couldn't process the URL, since URIjs choked on it.\n        return false;\n    }\n\n    // simplecrawler uses slightly different terminology to URIjs. Sorry!\n    return {\n        host:      newUrl.hostname(),\n        path:      newUrl.resource(),\n        port:      newUrl.port(),\n        protocol:  newUrl.protocol() || \"http\",\n        uriPath:   newUrl.path(),\n        url:       newUrl.href(),\n        depth:     referrer.depth + 1,\n        referrer:  referrer.url,\n        fetched:   false,\n        status:    \"created\",\n        stateData: {}\n    };\n};\n\n/**\n * Performs string replace operations on a URL string. Eg. removes HTML\n * attribute fluff around actual URL, replaces leading \"//\" with absolute\n * protocol etc.\n * @private\n * @param  {String} URL          The URL to be cleaned\n * @param  {QueueItem} queueItem The queue item representing the resource where this URL was discovered\n * @return {String}              Returns the cleaned URL\n */\nfunction cleanURL (URL, queueItem) {\n    return URL\n        .replace(/^(?:\\s*href|\\s*src)\\s*=+\\s*/i, \"\")\n        .replace(/^\\s*/, \"\")\n        .replace(/^(['\"])(.*)\\1$/, \"$2\")\n        .replace(/^url\\((.*)\\)/i, \"$1\")\n        .replace(/^javascript:\\s*(\\w*\\(['\"](.*)['\"]\\))*.*/i, \"$2\")\n        .replace(/^(['\"])(.*)\\1$/, \"$2\")\n        .replace(/^\\((.*)\\)$/, \"$1\")\n        .replace(/^\\/\\//, queueItem.protocol + \"://\")\n        .replace(/&amp;/gi, \"&\")\n        .replace(/&#38;/gi, \"&\")\n        .replace(/&#x00026;/gi, \"&\")\n        .split(\"#\")\n        .shift()\n        .trim();\n}\n\n/**\n * Cleans a list of resources, usually provided by\n * {@link Crawler#discoverResources}. Also makes relative URL's absolute to the\n * URL of the queueItem argument.\n * @param  {Array} urlMatch      An array of URL's\n * @param  {QueueItem} queueItem The queue item representing the resource where the URL's were discovered\n * @return {Array}               Returns an array of unique and absolute URL's\n */\nCrawler.prototype.cleanExpandResources = function(urlMatch, queueItem) {\n    var crawler = this;\n\n    if (!urlMatch) {\n        return [];\n    }\n\n    return urlMatch\n        .filter(Boolean)\n        .map(function(url) {\n            return cleanURL(url, queueItem);\n        })\n        .reduce(function(list, URL) {\n\n            // Ensure URL is whole and complete\n            try {\n                URL = uri(URL)\n                    .absoluteTo(queueItem.url || \"\")\n                    .normalize()\n                    .href();\n            } catch (e) {\n                // But if URI.js couldn't parse it - nobody can!\n                return list;\n            }\n\n            // If we hit an empty item, don't return it\n            if (!URL.length) {\n                return list;\n            }\n\n            // If we don't support the protocol in question\n            if (!crawler.protocolSupported(URL)) {\n                return list;\n            }\n\n            // Does the item already exist in the list?\n            var exists = list.some(function(entry) {\n                return entry === URL;\n            });\n\n            if (exists) {\n                return list;\n            }\n\n            return list.concat(URL);\n        }, []);\n};\n\n/**\n * Discovers linked resources in an HTML, XML or text document.\n * @param  {String} resourceText The body of the text document that is to be searched for resources\n * @return {Array}               Returns the array of discovered URL's. It is not the responsibility of this method to clean this array of duplicates etc. That's what {@link Crawler#cleanExpandResources} is for.\n */\nCrawler.prototype.discoverResources = function(resourceText) {\n    var crawler = this;\n\n    if (!crawler.parseHTMLComments) {\n        resourceText = resourceText.replace(/<!--([\\s\\S]+?)-->/g, \"\");\n    }\n\n    if (!crawler.parseScriptTags) {\n        resourceText = resourceText.replace(/<script(.*?)>([\\s\\S]*?)<\\/script>/gi, \"\");\n    }\n\n    if (crawler.respectRobotsTxt && /<meta(?:\\s[^>]*)?\\sname\\s*=\\s*[\"']?robots[\"']?[^>]*>/i.test(resourceText)) {\n        var robotsValue = /<meta(?:\\s[^>]*)?\\svalue\\s*=\\s*[\"']?([\\w\\s,]+)[\"']?[^>]*>/i.exec(resourceText.toLowerCase());\n\n        if (Array.isArray(robotsValue) && /nofollow/i.test(robotsValue[1])) {\n            return [];\n        }\n    }\n\n    // Rough scan for URLs\n    return crawler.discoverRegex.reduce(function(list, extracter) {\n        var resources;\n\n        if (extracter instanceof Function) {\n            resources = extracter(resourceText);\n        } else {\n            resources = resourceText.match(extracter);\n        }\n\n        return resources ? list.concat(resources) : list;\n    }, []);\n};\n\n/**\n * Determines whether a domain is valid for crawling based on configurable\n * rules.\n * @param  {String} host The domain name that's a candidate for fetching\n * @return {Boolean}     Returns true if the crawler if allowed to fetch resources from the domain, false if not.\n */\nCrawler.prototype.domainValid = function(host) {\n    var crawler = this;\n\n    // If we're ignoring the WWW domain, remove the WWW for comparisons...\n    if (crawler.ignoreWWWDomain) {\n        host = host.replace(/^www\\./i, \"\");\n    }\n\n    function domainInWhitelist(host) {\n\n        // If there's no whitelist, or the whitelist is of zero length,\n        // just return false.\n        if (!crawler.domainWhitelist || !crawler.domainWhitelist.length) {\n            return false;\n        }\n\n        // Otherwise, scan through it.\n        return crawler.domainWhitelist.some(function(entry) {\n            // If the domain is just equal, return true.\n            if (host === entry) {\n                return true;\n            }\n            // If we're ignoring WWW subdomains, and both domains,\n            // less www. are the same, return true.\n            if (crawler.ignoreWWWDomain && host === entry.replace(/^www\\./i, \"\")) {\n                return true;\n            }\n            return false;\n        });\n    }\n\n    // Checks if the first domain is a subdomain of the second\n    function isSubdomainOf(subdomain, host) {\n\n        // Comparisons must be case-insensitive\n        subdomain   = subdomain.toLowerCase();\n        host        = host.toLowerCase();\n\n        // If we're ignoring www, remove it from both\n        // (if www is the first domain component...)\n        if (crawler.ignoreWWWDomain) {\n            subdomain = subdomain.replace(/^www./ig, \"\");\n            host = host.replace(/^www./ig, \"\");\n        }\n\n        // They should be the same flipped around!\n        return subdomain.split(\"\").reverse().join(\"\").substr(0, host.length) ===\n                host.split(\"\").reverse().join(\"\");\n    }\n\n           // If we're not filtering by domain, just return true.\n    return !crawler.filterByDomain ||\n           // Or if the domain is just the right one, return true.\n           host === crawler.host ||\n           // Or if we're ignoring WWW subdomains, and both domains,\n           // less www. are the same, return true.\n           crawler.ignoreWWWDomain &&\n               crawler.host.replace(/^www\\./i, \"\") ===\n                   host.replace(/^www\\./i, \"\") ||\n           // Or if the domain in question exists in the domain whitelist,\n           // return true.\n           domainInWhitelist(host) ||\n           // Or if we're scanning subdomains, and this domain is a subdomain\n           // of the crawler's set domain, return true.\n           crawler.scanSubdomains && isSubdomainOf(host, crawler.host);\n};\n\n/**\n * Initiates discovery of linked resources in an HTML or text document, and\n * queues the resources if applicable. Not to be confused with\n * {@link Crawler#discoverResources}, despite that method being the main\n * component of this one, since this method queues the resources in addition to\n * discovering them.\n * @fires  Crawler#discoverycomplete\n * @param  {String|Buffer} resourceData The document body to search for URL's\n * @param  {QueueItem} queueItem        The queue item that represents the fetched document body\n * @return {Crawler}                    Returns the crawler instance to enable chained API calls\n */\nCrawler.prototype.queueLinkedItems = function(resourceData, queueItem) {\n    var crawler = this;\n\n    var resources = crawler.discoverResources(resourceData.toString(), queueItem);\n    resources = crawler.cleanExpandResources(resources, queueItem);\n\n    /**\n     * Fired when a request times out\n     * @event Crawler#fetchtimeout\n     * @param {QueueItem} queueItem The queue item for which the request timed out\n     * @param {Number} timeout      The delay in milliseconds after which the request timed out\n     */\n    crawler.emit(\"discoverycomplete\", queueItem, resources);\n\n    resources.forEach(function(url) {\n        if (crawler.maxDepth === 0 || queueItem.depth + 1 <= crawler.maxDepth) {\n            crawler.queueURL(url, queueItem);\n        }\n    });\n\n    return crawler;\n};\n\n/**\n * Queues a URL for fetching after cleaning, validating and constructing a queue\n * item from it. If you're queueing a URL manually, use this method rather than\n * {@link Crawler#queue#add}\n * @fires Crawler#invaliddomain\n * @fires Crawler#fetchdisallowed\n * @fires Crawler#fetchconditionerror\n * @fires Crawler#fetchprevented\n * @fires Crawler#queueduplicate\n * @fires Crawler#queueerror\n * @fires Crawler#queueadd\n * @param {String} url            An absolute or relative URL. If relative, {@link Crawler#processURL} will make it absolute to the referrer queue item.\n * @param {QueueItem} [referrer]  The queue item representing the resource where this URL was discovered.\n * @param {Boolean} [force]       If true, the URL will be queued regardless of whether it already exists in the queue or not.\n * @return {Boolean}              The return value used to indicate whether the URL passed all fetch conditions and robots.txt rules. With the advent of async fetch conditions, the return value will no longer take fetch conditions into account.\n */\nCrawler.prototype.queueURL = function(url, referrer, force) {\n    var crawler = this,\n        queueItem = typeof url === \"object\" ? url : crawler.processURL(url, referrer);\n\n    // URL Parser decided this URL was junky. Next please!\n    if (!queueItem) {\n        return false;\n    }\n\n    // Check that the domain is valid before adding it to the queue\n    if (!crawler.domainValid(queueItem.host)) {\n        /**\n         * Fired when a resource wasn't queued because of an invalid domain name\n         * @event Crawler#invaliddomain\n         * @param {QueueItem} queueItem The queue item representing the disallowed URL\n         */\n        crawler.emit(\"invaliddomain\", queueItem);\n        return false;\n    }\n\n    if (!crawler.urlIsAllowed(queueItem.url)) {\n        /**\n         * Fired when a resource wasn't queued because it was disallowed by the\n         * site's robots.txt rules\n         * @event Crawler#fetchdisallowed\n         * @param {QueueItem} queueItem The queue item representing the disallowed URL\n         */\n        crawler.emit(\"fetchdisallowed\", queueItem);\n        return false;\n    }\n\n    async.every(crawler._fetchConditions, function(fetchCondition, callback) {\n        if (fetchCondition.length < 3) {\n            try {\n                callback(null, fetchCondition(queueItem, referrer));\n            } catch (error) {\n                callback(error);\n            }\n        } else {\n            fetchCondition(queueItem, referrer, callback);\n        }\n    }, function(error, result) {\n        if (error) {\n            /**\n             * Fired when a fetch condition returns an error\n             * @event Crawler#fetchconditionerror\n             * @param {QueueItem} queueItem The queue item that was processed when the error was encountered\n             * @param {*}         error\n             */\n            crawler.emit(\"fetchconditionerror\", queueItem, error);\n            return false;\n        }\n\n        if (!result) {\n            /**\n             * Fired when a fetch condition prevented the queueing of a URL\n             * @event Crawler#fetchprevented\n             * @param {QueueItem} queueItem      The queue item that didn't pass the fetch conditions\n             * @param {Function}  fetchCondition The first fetch condition that returned false\n             */\n            crawler.emit(\"fetchprevented\", queueItem);\n            return false;\n        }\n\n        crawler.queue.add(queueItem, force, function(error) {\n            if (error) {\n                if (error.code && error.code === \"DUPLICATE\") {\n                    /**\n                     * Fired when a new queue item was rejected because another\n                     * queue item with the same URL was already in the queue\n                     * @event Crawler#queueduplicate\n                     * @param {QueueItem} queueItem The queue item that was rejected\n                     */\n                    return crawler.emit(\"queueduplicate\", queueItem);\n                }\n\n                /**\n                 * Fired when an error was encountered while updating a queue item\n                 * @event Crawler#queueerror\n                 * @param {QueueItem} error     The error that was returned by the queue\n                 * @param {QueueItem} queueItem The queue item that the crawler tried to update when it encountered the error\n                 */\n                return crawler.emit(\"queueerror\", error, queueItem);\n            }\n\n            /**\n             * Fired when an item was added to the crawler's queue\n             * @event Crawler#queueadd\n             * @param {QueueItem} queueItem The queue item that was added to the queue\n             * @param {QueueItem} referrer  The queue item representing the resource where the new queue item was found\n             */\n            crawler.emit(\"queueadd\", queueItem, referrer);\n        });\n    });\n\n    return true;\n};\n\n/**\n * Handles the initial fetching of a queue item. Once an initial response has\n * been received, {@link Crawler#handleResponse} will handle the downloading of\n * the resource data\n * @fires  Crawler#fetchstart\n * @fires  Crawler#fetchtimeout\n * @fires  Crawler#fetchclienterror\n * @param  {QueueItem} queueItem The queue item that will be fetched\n * @return {Crawler}             Returns the crawler instance to enable chained API calls\n */\nCrawler.prototype.fetchQueueItem = function(queueItem) {\n    var crawler = this;\n\n    crawler.queue.update(queueItem.id, {\n        status: \"spooled\"\n    }, function(error, queueItem) {\n        if (error) {\n            return crawler.emit(\"queueerror\", error, queueItem);\n        }\n\n        var client = queueItem.protocol === \"https\" ? https : http,\n            agent  = queueItem.protocol === \"https\" ? crawler.httpsAgent : crawler.httpAgent;\n\n        if (agent.maxSockets < crawler.maxConcurrency) {\n            agent.maxSockets = crawler.maxConcurrency;\n        }\n\n        if (client === https && crawler.ignoreInvalidSSL === true) {\n            client.rejectUnauthorized = false;\n            client.strictSSL = false;\n        }\n\n        var requestOptions = crawler.getRequestOptions(queueItem),\n            timeCommenced = Date.now();\n\n        var clientRequest = client.request(requestOptions, function(response) {\n            crawler.handleResponse(queueItem, response, timeCommenced);\n        });\n\n        clientRequest.end();\n\n        // Enable central tracking of this request\n        crawler._openRequests.push(clientRequest);\n\n        // Ensure the request is removed from the tracking array if it is\n        // forcibly aborted\n        clientRequest.on(\"abort\", function() {\n            if (crawler._openRequests.indexOf(clientRequest) > -1) {\n                crawler._openRequests.splice(\n                    crawler._openRequests.indexOf(clientRequest), 1);\n            }\n        });\n\n        clientRequest.setTimeout(crawler.timeout, function() {\n            if (queueItem.fetched) {\n                return;\n            }\n\n            if (crawler.running && !queueItem.fetched) {\n                // Remove this request from the open request map\n                crawler._openRequests.splice(\n                    crawler._openRequests.indexOf(clientRequest), 1);\n            }\n\n            crawler.queue.update(queueItem.id, {\n                fetched: true,\n                status: \"timeout\"\n            }, function(error, queueItem) {\n                if (error) {\n                    return crawler.emit(\"queueerror\", error, queueItem);\n                }\n\n                /**\n                 * Fired when a request times out\n                 * @event Crawler#fetchtimeout\n                 * @param {QueueItem} queueItem The queue item for which the request timed out\n                 * @param {Number} timeout      The delay in milliseconds after which the request timed out\n                 */\n                crawler.emit(\"fetchtimeout\", queueItem, crawler.timeout);\n                clientRequest.abort();\n            });\n        });\n\n        clientRequest.on(\"error\", function(errorData) {\n\n            // This event will be thrown if we manually aborted the request,\n            // but we don't want to do anything in that case.\n            if (clientRequest.aborted) {\n                return;\n            }\n\n            if (crawler.running && !queueItem.fetched) {\n                // Remove this request from the open request map\n                crawler._openRequests.splice(\n                    crawler._openRequests.indexOf(clientRequest), 1);\n            }\n\n            crawler.queue.update(queueItem.id, {\n                fetched: true,\n                status: \"failed\",\n                stateData: {\n                    code: 600\n                }\n            }, function(error, queueItem) {\n                if (error) {\n                    return crawler.emit(\"queueerror\", error, queueItem);\n                }\n\n                /**\n                 * Fired when a request encounters an unknown error\n                 * @event Crawler#fetchclienterror\n                 * @param {QueueItem} queueItem The queue item for which the request has errored\n                 * @param {Object} error        The error supplied to the `error` event on the request\n                 */\n                crawler.emit(\"fetchclienterror\", queueItem, errorData);\n            });\n        });\n\n        /**\n         * Fired just after a request has been initiated\n         * @event Crawler#fetchstart\n         * @param {QueueItem} queueItem   The queue item for which the request has been initiated\n         * @param {Object} requestOptions The options generated for the HTTP request\n         */\n        crawler.emit(\"fetchstart\", queueItem, requestOptions);\n    });\n\n    return crawler;\n};\n\n/**\n * Decodes a string buffer based on a complete Content-Type header. Will also\n * look for an embedded <meta> tag with a charset definition, but the\n * Content-Type header is prioritized, see the [MDN documentation]{@link https://developer.mozilla.org/en-US/docs/Web/HTML/Element/meta#attr-charset}\n * for more details.\n * @param  {Buffer} buffer              A response buffer\n * @param  {String} [contentTypeHeader] ContentType header received from HTTP request\n * @return {String}                     The decoded buffer contents\n */\nCrawler.prototype.decodeBuffer = function(buffer, contentTypeHeader) {\n    contentTypeHeader = contentTypeHeader || \"\";\n\n    var embeddedEncoding = /<meta[^>]*charset\\s*=\\s*[\"']?([\\w\\-]*)/i.exec(buffer.toString(undefined, 0, 512)) || [],\n        encoding = contentTypeHeader.split(\"charset=\")[1] || embeddedEncoding[1] || contentTypeHeader;\n\n    encoding = iconv.encodingExists(encoding) ? encoding : \"utf8\";\n\n    return iconv.decode(buffer, encoding);\n};\n\n/**\n * Handles downloading of a resource after an initial HTTP response has been\n * received.\n * @fires  Crawler#fetchheaders\n * @fires  Crawler#fetchcomplete\n * @fires  Crawler#fetchdataerror\n * @fires  Crawler#notmodified\n * @fires  Crawler#fetchredirect\n * @fires  Crawler#fetch404\n * @fires  Crawler#fetcherror\n * @param  {QueueItem} queueItem             A queue item representing the resource to be fetched\n * @param  {http.IncomingMessage} response   An instace of [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage}\n * @param  {Date} [timeCommenced=Date.now()] Specifies at what time the request was initiated\n * @return {Crawler}                         Returns the crawler instance to enable chained API calls\n */\nCrawler.prototype.handleResponse = function(queueItem, response, timeCommenced) {\n    var crawler = this,\n        dataReceived = false,\n        timeHeadersReceived = Date.now(),\n        timeDataReceived,\n        redirectQueueItem,\n        responseBuffer,\n        responseLength,\n        responseLengthReceived = 0,\n        contentType = response.headers[\"content-type\"];\n\n    timeCommenced = timeCommenced || Date.now();\n    responseLength = parseInt(response.headers[\"content-length\"], 10);\n    responseLength = !isNaN(responseLength) ? responseLength : 0;\n\n    crawler.queue.update(queueItem.id, {\n        stateData: {\n            requestLatency: timeHeadersReceived - timeCommenced,\n            requestTime: timeHeadersReceived - timeCommenced,\n            contentLength: responseLength,\n            contentType: contentType,\n            code: response.statusCode,\n            headers: response.headers\n        }\n    }, function(error, queueItem) {\n        if (error) {\n            return crawler.emit(\"queueerror\", error, queueItem);\n        }\n\n        // Do we need to save cookies? Were we sent any?\n        if (crawler.acceptCookies && response.headers.hasOwnProperty(\"set-cookie\")) {\n            try {\n                crawler.cookies.addFromHeaders(response.headers[\"set-cookie\"]);\n            } catch (error) {\n                /**\n                 * Fired when an error was encountered while trying to add a\n                 * cookie to the cookie jar\n                 * @event Crawler#cookieerror\n                 * @param {QueueItem} queueItem The queue item representing the resource that returned the cookie\n                 * @param {Error} error         The error that was encountered\n                 * @param {String} cookie       The Set-Cookie header value that was returned from the request\n                 */\n                crawler.emit(\"cookieerror\", queueItem, error, response.headers[\"set-cookie\"]);\n            }\n        }\n\n        /**\n         * Fired when the headers for a request have been received\n         * @event Crawler#fetchheaders\n         * @param {QueueItem} queueItem           The queue item for which the headers have been received\n         * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n         */\n        crawler.emit(\"fetchheaders\", queueItem, response);\n\n        // We already know that the response will be too big\n        if (responseLength > crawler.maxResourceSize) {\n\n            crawler.queue.update(queueItem.id, {\n                fetched: true\n            }, function(error, queueItem) {\n                if (error) {\n                    return crawler.emit(\"queueerror\", error, queueItem);\n                }\n\n                // Remove this request from the open request map\n                crawler._openRequests.splice(\n                    crawler._openRequests.indexOf(response.req), 1);\n\n                response.destroy();\n                crawler.emit(\"fetchdataerror\", queueItem, response);\n            });\n\n        // We should just go ahead and get the data\n        } else if (response.statusCode >= 200 && response.statusCode < 300) {\n\n            async.every(crawler._downloadConditions, function(downloadCondition, callback) {\n                if (downloadCondition.length < 3) {\n                    try {\n                        callback(null, downloadCondition(queueItem, response));\n                    } catch (error) {\n                        callback(error);\n                    }\n                } else {\n                    downloadCondition(queueItem, response, callback);\n                }\n            }, function(error, result) {\n\n                if (error) {\n                    /**\n                     * Fired when a download condition returns an error\n                     * @event Crawler#downloadconditionerror\n                     * @param {QueueItem} queueItem The queue item that was processed when the error was encountered\n                     * @param {*}         error\n                     */\n                    crawler.emit(\"downloadconditionerror\", queueItem, error);\n                    return false;\n                }\n\n                if (!result) {\n                    crawler.queue.update(queueItem.id, {\n                        fetched: true,\n                        status: \"downloadprevented\"\n                    }, function(error, queueItem) {\n                        crawler._openRequests.splice(\n                            crawler._openRequests.indexOf(response.req), 1);\n\n                        response.destroy();\n                        /**\n                         * Fired when the downloading of a resource was prevented\n                         * by a download condition\n                         * @event Crawler#downloadprevented\n                         * @param {QueueItem} queueItem           The queue item representing the resource that was halfway fetched\n                         * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                         */\n                        crawler.emit(\"downloadprevented\", queueItem, response);\n                    });\n\n                } else {\n                    crawler.queue.update(queueItem.id, {\n                        status: \"headers\"\n                    }, function(error, queueItem) {\n                        if (error) {\n                            return crawler.emit(\"queueerror\", error, queueItem);\n                        }\n\n                        // Create a buffer with our response length\n                        responseBuffer = new Buffer(responseLength);\n\n                        // Only if we're prepared to download non-text resources...\n                        if (crawler.downloadUnsupported || crawler.mimeTypeSupported(contentType)) {\n                            response.on(\"data\", receiveData);\n                            response.on(\"end\", processReceivedData);\n                        } else {\n                            crawler.queue.update(queueItem.id, {\n                                fetched: true\n                            }, function() {\n                                // Remove this request from the open request map\n                                crawler._openRequests.splice(\n                                    crawler._openRequests.indexOf(response.req), 1);\n\n                                response.destroy();\n                            });\n                        }\n\n                        crawler._isFirstRequest = false;\n                    });\n                }\n            });\n\n        // We've got a not-modified response back\n        } else if (response.statusCode === 304) {\n\n            if (crawler.cache !== null && crawler.cache.getCacheData) {\n                // We've got access to a cache\n                crawler.cache.getCacheData(queueItem, function(cacheObject) {\n                    crawler.emit(\"notmodified\", queueItem, response, cacheObject);\n                });\n            } else {\n                /**\n                 * Fired when the crawler's cache was enabled and the server responded with a 304 Not Modified status for the request\n                 * @event Crawler#notmodified\n                 * @param {QueueItem} queueItem           The queue item for which the request returned a 304 status\n                 * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                 * @param {CacheObject} cacheObject       The CacheObject returned from the cache backend\n                 */\n                crawler.emit(\"notmodified\", queueItem, response);\n            }\n\n            response.destroy();\n            // Remove this request from the open request map\n            crawler._openRequests.splice(\n                crawler._openRequests.indexOf(response.req), 1);\n\n            crawler._isFirstRequest = false;\n\n        // If we should queue a redirect\n        } else if (response.statusCode >= 300 && response.statusCode < 400 && response.headers.location) {\n\n            crawler.queue.update(queueItem.id, {\n                fetched: true,\n                status: \"redirected\"\n            }, function(error, queueItem) {\n\n                // Parse the redirect URL ready for adding to the queue...\n                redirectQueueItem = crawler.processURL(response.headers.location, queueItem);\n\n                /**\n                 * Fired when the server returned a redirect HTTP status for the request\n                 * @event Crawler#fetchredirect\n                 * @param {QueueItem} queueItem           The queue item for which the request was redirected\n                 * @param {QueueItem} redirectQueueItem   The queue item for the redirect target resource\n                 * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                 */\n                crawler.emit(\"fetchredirect\", queueItem, redirectQueueItem, response);\n\n                if (crawler._isFirstRequest) {\n                    redirectQueueItem.depth = 1;\n                }\n\n                if (crawler.allowInitialDomainChange && crawler._isFirstRequest) {\n                    crawler.host = redirectQueueItem.host;\n                }\n\n                crawler.queueURL(redirectQueueItem, queueItem);\n                response.destroy();\n\n                // Remove this request from the open request map\n                crawler._openRequests.splice(\n                    crawler._openRequests.indexOf(response.req), 1);\n            });\n\n        // Ignore this request, but record that we had a 404\n        } else if (response.statusCode === 404 || response.statusCode === 410) {\n\n            crawler.queue.update(queueItem.id, {\n                fetched: true,\n                status: \"notfound\"\n            }, function(error, queueItem) {\n                /**\n                 * Fired when the server returned a 404 Not Found status for the request\n                 * @event Crawler#fetch404\n                 * @param {QueueItem} queueItem           The queue item for which the request returned a 404 status\n                 * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                 */\n                /**\n                 * Fired when the server returned a 410 Gone status for the request\n                 * @event Crawler#fetch410\n                 * @param {QueueItem} queueItem           The queue item for which the request returned a 410 status\n                 * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                 */\n                crawler.emit(\"fetch\" + response.statusCode, queueItem, response);\n                response.destroy();\n\n                // Remove this request from the open request map\n                crawler._openRequests.splice(\n                    crawler._openRequests.indexOf(response.req), 1);\n\n                crawler._isFirstRequest = false;\n            });\n\n        // And oh dear. Handle this one as well. (other 400s, 500s, etc)\n        } else {\n\n            crawler.queue.update(queueItem.id, {\n                fetched: true,\n                status: \"failed\"\n            }, function(error, queueItem) {\n                /**\n                 * Fired when the server returned a status code above 400 that isn't 404 or 410\n                 * @event Crawler#fetcherror\n                 * @param {QueueItem} queueItem           The queue item for which the request failed\n                 * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                 */\n                crawler.emit(\"fetcherror\", queueItem, response);\n                response.destroy();\n\n                // Remove this request from the open request map\n                crawler._openRequests.splice(\n                    crawler._openRequests.indexOf(response.req), 1);\n\n                crawler._isFirstRequest = false;\n            });\n\n        }\n\n\n        function emitFetchComplete(responseBody, decompressedBuffer) {\n            crawler.queue.update(queueItem.id, {\n                fetched: true,\n                status: \"downloaded\"\n            }, function(error, queueItem) {\n                // Remove this request from the open request map\n                crawler._openRequests.splice(\n                    crawler._openRequests.indexOf(response.req), 1);\n\n                if (error) {\n                    return crawler.emit(\"queueerror\", error, queueItem);\n                }\n\n                if (crawler.decodeResponses) {\n                    responseBody = crawler.decodeBuffer(responseBody, queueItem.stateData.contentType);\n                }\n\n                /**\n                 * Fired when the request has completed\n                 * @event Crawler#fetchcomplete\n                 * @param {QueueItem} queueItem           The queue item for which the request has completed\n                 * @param {String|Buffer} responseBody    If Crawler.decodeResponses is true, this will be the decoded HTTP response. Otherwise it will be the raw response buffer.\n                 * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                 */\n                crawler.emit(\"fetchcomplete\", queueItem, responseBody, response);\n\n                // We only process the item if it's of a valid mimetype\n                // and only if the crawler is set to discover its own resources\n                if (crawler.mimeTypeSupported(contentType) && crawler.discoverResources) {\n                    crawler.queueLinkedItems(decompressedBuffer || responseBody, queueItem);\n                }\n            });\n        }\n\n        // Function for dealing with 200 responses\n        function processReceivedData() {\n            if (dataReceived || queueItem.fetched) {\n                return;\n            }\n\n            responseBuffer = responseBuffer.slice(0, responseLengthReceived);\n            dataReceived = true;\n            timeDataReceived = Date.now();\n\n            crawler.queue.update(queueItem.id, {\n                stateData: {\n                    downloadTime: timeDataReceived - timeHeadersReceived,\n                    requestTime: timeDataReceived - timeCommenced,\n                    actualDataSize: responseBuffer.length,\n                    sentIncorrectSize: responseBuffer.length !== responseLength\n                }\n            }, function (error, queueItem) {\n                if (error) {\n                    // Remove this request from the open request map\n                    crawler._openRequests.splice(\n                        crawler._openRequests.indexOf(response.req), 1);\n\n                    return crawler.emit(\"queueerror\", error, queueItem);\n                }\n\n                // First, save item to cache (if we're using a cache!)\n                if (crawler.cache && crawler.cache.setCacheData instanceof Function) {\n                    crawler.cache.setCacheData(queueItem, responseBuffer);\n                }\n\n                // No matter the value of `crawler.decompressResponses`, we still\n                // decompress the response if it's gzipped or deflated. This is\n                // because we always provide the discoverResources method with a\n                // decompressed buffer\n                if (/(gzip|deflate)/.test(queueItem.stateData.headers[\"content-encoding\"])) {\n                    zlib.unzip(responseBuffer, function(error, decompressedBuffer) {\n                        if (error) {\n                            /**\n                             * Fired when an error was encountered while unzipping the response data\n                             * @event Crawler#gziperror\n                             * @param {QueueItem} queueItem           The queue item for which the unzipping failed\n                             * @param {String|Buffer} responseBody    If Crawler.decodeResponses is true, this will be the decoded HTTP response. Otherwise it will be the raw response buffer.\n                             * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                             */\n                            crawler.emit(\"gziperror\", queueItem, error, responseBuffer);\n                            emitFetchComplete(responseBuffer);\n                        } else {\n                            var responseBody = crawler.decompressResponses ? decompressedBuffer : responseBuffer;\n                            emitFetchComplete(responseBody, decompressedBuffer);\n                        }\n                    });\n                } else {\n                    emitFetchComplete(responseBuffer);\n                }\n            });\n        }\n\n        function receiveData(chunk) {\n            if (!chunk.length || dataReceived) {\n                return;\n            }\n\n            if (responseLengthReceived + chunk.length > responseBuffer.length) {\n\n                // Oh dear. We've been sent more data than we were initially told.\n                // This could be a mis-calculation, or a streaming resource.\n                // Let's increase the size of our buffer to match, as long as it isn't\n                // larger than our maximum resource size.\n                if (responseLengthReceived + chunk.length <= crawler.maxResourceSize) {\n\n                    // Create a temporary buffer with the new response length, copy\n                    // the old data into it and replace the old buffer with it\n                    var tmpNewBuffer = new Buffer(responseLengthReceived + chunk.length);\n                    responseBuffer.copy(tmpNewBuffer, 0, 0, responseBuffer.length);\n                    chunk.copy(tmpNewBuffer, responseBuffer.length, 0, chunk.length);\n                    responseBuffer = tmpNewBuffer;\n                } else {\n\n                    // The response size exceeds maxResourceSize. Throw event and\n                    // ignore. We'll then deal with the data that we have.\n                    response.destroy();\n\n                    /**\n                     * Fired when a resource couldn't be downloaded because it exceeded the maximum allowed size\n                     * @event Crawler#fetchdataerror\n                     * @param {QueueItem} queueItem           The queue item for which the request failed\n                     * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage} for the request's response\n                     */\n                    crawler.emit(\"fetchdataerror\", queueItem, response);\n                }\n            } else {\n                chunk.copy(responseBuffer, responseLengthReceived, 0, chunk.length);\n            }\n\n            responseLengthReceived += chunk.length;\n        }\n    });\n\n\n    return crawler;\n};\n\n/**\n * The main crawler runloop. Fires at the interval specified in the crawler\n * configuration, when the crawl is running. May be manually fired. This\n * function initiates fetching of a queue item if there are enough workers to do\n * so and there are unfetched items in the queue.\n * @fires Crawler#robotstxterror\n * @fires Crawler#fetchdisallowed\n * @fires Crawler#complete\n * @return {Crawler} Returns the crawler instance to enable chained API calls\n */\nCrawler.prototype.crawl = function() {\n    var crawler = this;\n\n    if (crawler._openRequests.length >= crawler.maxConcurrency ||\n        crawler.fetchingRobotsTxt) {\n        return crawler;\n    }\n\n    crawler.queue.oldestUnfetchedItem(function(error, queueItem) {\n        if (error) {\n            // Do nothing\n        } else if (queueItem) {\n\n            var url = uri(queueItem.url).normalize();\n            var host = uri({\n                protocol: url.protocol(),\n                hostname: url.hostname(),\n                port: url.port()\n            }).href();\n\n            if (crawler.respectRobotsTxt && crawler._touchedHosts.indexOf(host) === -1) {\n                crawler._touchedHosts.push(host);\n                crawler.fetchingRobotsTxt = true;\n\n                var robotsTxtUrl = uri(host).pathname(\"/robots.txt\").href();\n\n                crawler.getRobotsTxt(robotsTxtUrl, function(error, robotsTxtUrl, robotsTxtBody) {\n                    if (error) {\n                        /**\n                         * Fired when an error was encountered while retrieving a robots.txt file\n                         * @event Crawler#robotstxterror\n                         * @param {Error} error The error returned from {@link Crawler#getRobotsTxt}\n                         */\n                        crawler.emit(\"robotstxterror\", error);\n                    } else {\n                        crawler._robotsTxts.push(robotsTxtParser(robotsTxtUrl, robotsTxtBody));\n                    }\n\n                    crawler.fetchingRobotsTxt = false;\n\n                    // It could be that the first URL we queued for any particular\n                    // host is in fact disallowed, so we double check once we've\n                    // fetched the robots.txt\n                    if (crawler.urlIsAllowed(queueItem.url)) {\n                        crawler.fetchQueueItem(queueItem);\n                    } else {\n                        crawler.queue.update(queueItem.id, {\n                            fetched: true,\n                            status: \"disallowed\"\n                        }, function(error, queueItem) {\n                            crawler.emit(\"fetchdisallowed\", queueItem);\n                        });\n                    }\n                });\n            } else {\n\n                crawler.fetchQueueItem(queueItem);\n            }\n        } else if (!crawler._openRequests.length && !crawler._openListeners) {\n\n            crawler.queue.countItems({ fetched: true }, function(err, completeCount) {\n                if (err) {\n                    throw err;\n                }\n\n                crawler.queue.getLength(function(err, length) {\n                    if (err) {\n                        throw err;\n                    }\n\n                    if (completeCount === length) {\n                        /**\n                         * Fired when the crawl has completed - all resources in the queue have been dealt with\n                         * @event Crawler#complete\n                         */\n                        crawler.emit(\"complete\");\n                        crawler.stop();\n                    }\n                });\n            });\n        }\n    });\n\n    return crawler;\n};\n\n/**\n * Stops the crawler by terminating the crawl runloop\n * @param  {Boolean} [abortRequestsInFlight=false] If true, will terminate all in-flight requests immediately\n * @return {Crawler}                               Returns the crawler instance to enable chained API calls\n */\nCrawler.prototype.stop = function(abortRequestsInFlight) {\n    var crawler = this;\n    clearInterval(crawler.crawlIntervalID);\n    crawler.running = false;\n\n    // If we've been asked to terminate the existing requests, do that now.\n    if (abortRequestsInFlight) {\n        crawler._openRequests.forEach(function(request) {\n            request.abort();\n        });\n    }\n\n    return crawler;\n};\n\n/**\n * Holds the crawler in a 'running' state, preventing the `complete` event from\n * firing until the returned callback has been executed, or a predetermined\n * timeout (as specified by `crawler.listenerTTL`) has elapsed.\n * @return {Function} A callback function that will allow the crawler to continue once called\n */\nCrawler.prototype.wait = function() {\n    var crawler = this,\n        cleared = false,\n        timeout =\n            setTimeout(function() {\n                if (cleared) {\n                    return;\n                }\n                cleared = true;\n                crawler._openListeners--;\n            }, crawler.listenerTTL);\n\n    crawler._openListeners++;\n\n    return function() {\n        if (cleared) {\n            return;\n        }\n        cleared = true;\n        crawler._openListeners--;\n        clearTimeout(timeout);\n    };\n};\n\n/**\n * Adds a function to an array of functions, where each one is evaluated against\n * every request after the headers of the resource represented by the queue item\n * have been fetched. If any of the functions return false, the resource data\n * will not be downloaded.\n * @param  {Function} callback Function to be called when the headers of the resource represented by the queue item have been downloaded\n * @return {Number}            The index of the download condition in the download conditions array. This can later be used to remove the download condition.\n */\nCrawler.prototype.addDownloadCondition = function(callback) {\n    if (!(callback instanceof Function)) {\n        throw new Error(\"Download condition must be a function\");\n    }\n\n    this._downloadConditions.push(callback);\n    return this._downloadConditions.length - 1;\n};\n\n/**\n * Removes a download condition from the download conditions array.\n * @param  {Number} index The index of the download condition in the download conditions array. This was returned from {@link Crawler#addDownloadCondition}\n * @return {Boolean}      If the removal was successful, the method will return true. Otherwise, it will throw an error.\n */\nCrawler.prototype.removeDownloadCondition = function(index) {\n    var crawler = this;\n\n    if (index instanceof Function) {\n        var itemIndex = crawler._downloadConditions.indexOf(index);\n        return Boolean(crawler._downloadConditions.splice(itemIndex, 1));\n    } else if (typeof index === \"number\") {\n        return Boolean(crawler._downloadConditions.splice(index, 1));\n    }\n\n    throw new Error(\"Unable to find indexed download condition\");\n};\n\n/**\n * Adds a function to an array of functions, where each one is evaluated against\n * every queue item that the crawler attempts to queue. If any of these\n * functions returns false, the queue item will not be queued.\n * @param  {Function} callback Function to be called after resource discovery that's able to prevent queueing of resource\n * @return {Number}            The index of the fetch condition in the fetch conditions array. This can later be used to remove the fetch condition.\n */\nCrawler.prototype.addFetchCondition = function(callback) {\n    if (!(callback instanceof Function)) {\n        throw new Error(\"Fetch condition must be a function\");\n    }\n\n    this._fetchConditions.push(callback);\n    return this._fetchConditions.length - 1;\n};\n\n/**\n * Removes a fetch condition from the fetch conditions array.\n * @param  {Number} index The index of the fetch condition in the fetch conditions array. This was returned from {@link Crawler#addFetchCondition}\n * @return {Boolean}      If the removal was successful, the method will return true. Otherwise, it will throw an error.\n */\nCrawler.prototype.removeFetchCondition = function(index) {\n    var crawler = this;\n\n    if (index instanceof Function) {\n        var itemIndex = crawler._downloadConditions.indexOf(index);\n        return Boolean(crawler._fetchConditions.splice(itemIndex, 1));\n    } else if (typeof index === \"number\") {\n        return Boolean(crawler._fetchConditions.splice(index, 1));\n    }\n\n    throw new Error(\"Unable to find indexed fetch condition\");\n};\n\nmodule.exports = Crawler;\n","/home/travis/build/npmtest/node-npmtest-simplecrawler/node_modules/simplecrawler/lib/queue.js":"/**\n * @file simplecrawler's queue implementation. This also serves as a reference\n * for the queue interface, that can be implemented by third parties as well\n */\n\nvar fs   = require(\"fs\"),\n    util = require(\"util\");\n\n/**\n * Recursive function that compares immutable properties on two objects.\n * @private\n * @param {Object} a Source object that will be compared against\n * @param {Object} b Comparison object. The functions determines if all of this object's properties are the same on the first object.\n * @return {Boolean} Returns true if all of the properties on `b` matched a property on `a`. If not, it returns false.\n */\nfunction compare(a, b) {\n    for (var key in a) {\n        if (a.hasOwnProperty(key)) {\n\n            if (typeof a[key] !== typeof b[key]) {\n                return false;\n            }\n\n            if (typeof a[key] === \"object\") {\n                if (!compare(a[key], b[key])) {\n                    return false;\n                }\n            } else if (a[key] !== b[key]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n\n/**\n * Recursive function that takes two objects and updates the properties on the\n * first object based on the ones in the second. Basically, it's a recursive\n * version of Object.assign.\n */\nfunction deepAssign(object, source) {\n    for (var key in source) {\n        if (source.hasOwnProperty(key)) {\n            if (typeof object[key] === \"object\" && typeof source[key] === \"object\") {\n                deepAssign(object[key], source[key]);\n            } else {\n                object[key] = source[key];\n            }\n        }\n    }\n\n    return object;\n}\n\n/**\n * Creates a new queue\n * @class\n */\nvar FetchQueue = function() {\n    Array.call(this);\n\n    /**\n     * Speeds up {@link FetchQueue.oldestUnfetchedItem} by storing the index at\n     * which the latest oldest unfetched queue item was found.\n     * @name FetchQueue._oldestUnfetchedIndex\n     * @private\n     * @type {Number}\n     */\n    Object.defineProperty(this, \"_oldestUnfetchedIndex\", {\n        enumerable: false,\n        writable: true,\n        value: 0\n    });\n\n    /**\n     * Serves as a cache for what URL's have been fetched. Keys are URL's,\n     * values are booleans.\n     * @name FetchQueue._scanIndex\n     * @private\n     * @type {Object}\n     */\n    Object.defineProperty(this, \"_scanIndex\", {\n        enumerable: false,\n        writable: true,\n        value: {}\n    });\n\n    /**\n     * Controls what properties can be operated on with the\n     * {@link FetchQueue#min}, {@link FetchQueue#avg} and {@link FetchQueue#max}\n     * methods.\n     * @name FetchQueue._allowedStatistics\n     * @type {Array}\n     */\n    Object.defineProperty(this, \"_allowedStatistics\", {\n        enumerable: false,\n        writable: true,\n        value: [\n            \"actualDataSize\",\n            \"contentLength\",\n            \"downloadTime\",\n            \"requestLatency\",\n            \"requestTime\"\n        ]\n    });\n};\n\nutil.inherits(FetchQueue, Array);\n\n/**\n * Called when {@link FetchQueue#add} returns a result\n * @callback FetchQueue~addCallback\n * @param {Error} [error]         If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {QueueItem} [queueItem] The queue item that was added to the queue. It's status property will have changed to `\"queued\"`.\n */\n\n/**\n * Adds an item to the queue\n * @param {QueueItem} queueItem             Queue item that is to be added to the queue\n * @param {Boolean} [force=false]           If true, the queue item will be added regardless of whether it already exists in the queue\n * @param {FetchQueue~addCallback} callback\n */\nFetchQueue.prototype.add = function(queueItem, force, callback) {\n    var queue = this;\n\n    function addToQueue() {\n        queue._scanIndex[queueItem.url] = true;\n        queueItem.id = queue.length;\n        queueItem.status = \"queued\";\n        queue.push(queueItem);\n        callback(null, queueItem);\n    }\n\n    queue.exists(queueItem.url, function(err, exists) {\n        if (err) {\n            callback(err);\n        } else if (!exists) {\n            addToQueue();\n        } else if (force) {\n            if (queue.indexOf(queueItem) > -1) {\n                callback(new Error(\"Can't add a queueItem instance twice. You may create a new one from the same URL however.\"));\n            } else {\n                addToQueue();\n            }\n        } else {\n            var error = new Error(\"Resource already exists in queue!\");\n            error.code = \"DUPLICATE\";\n            callback(error);\n        }\n    });\n};\n\n/**\n * Called when {@link FetchQueue#exists} returns a result\n * @callback FetchQueue~existsCallback\n * @param {Error} [error]  If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Number} [count] The number of occurences in the queue of the provided URL.\n */\n\n/**\n * Checks if a URL already exists in the queue. Returns the number of occurences\n * of that URL.\n * @param {String} url                         URL to check the existence of in the queue\n * @param {FetchQueue~existsCallback} callback\n */\nFetchQueue.prototype.exists = function(url, callback) {\n    if (this._scanIndex[url]) {\n        callback(null, 1);\n    } else {\n        callback(null, 0);\n    }\n};\n\n/**\n * Called when {@link FetchQueue#get} returns a result\n * @callback FetchQueue~getCallback\n * @param {Error} [error]         If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {QueueItem} [queueItem] The queue item found at that index in the queue.\n */\n\n/**\n * Get a queue item by index\n * @param {Number} index                    The index of the queue item in the queue\n * @param {FetchQueue~getCallback} callback\n */\nFetchQueue.prototype.get = function(index, callback) {\n    var queue = this;\n\n    queue.getLength(function(error, length) {\n        if (error) {\n            callback(error);\n        } else if (index >= length) {\n            callback(new RangeError(\"Index was greater than the queue's length\"));\n        } else {\n            callback(null, queue[index]);\n        }\n    });\n};\n\n/**\n * Called when {@link FetchQueue#update} returns a result\n * @callback FetchQueue~updateCallback\n * @param {Error} [error]         If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {QueueItem} [queueItem] The updated queue item\n */\n\n/**\n * Updates a queue item in the queue.\n * @param {Number} id                          ID of the queue item that is to be updated\n * @param {Object} updates                     Object that will be deeply assigned (as in `Object.assign`) to the queue item. That means that nested objects will also be resursively assigned.\n * @param {FetchQueue~updateCallback} callback\n */\nFetchQueue.prototype.update = function (id, updates, callback) {\n    var queue = this,\n        queueItem;\n\n    for (var i = 0; i < queue.length; i++) {\n        if (queue[i].id === id) {\n            queueItem = queue[i];\n            break;\n        }\n    }\n\n    if (!queueItem) {\n        callback(new Error(\"No queueItem found with that URL\"));\n    } else {\n        deepAssign(queueItem, updates);\n        callback(null, queueItem);\n    }\n};\n\n/**\n * Called when {@link FetchQueue#oldestUnfetchedItem} returns a result\n * @callback FetchQueue~oldestUnfetchedItemCallback\n * @param {Error} [error]         If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {QueueItem} [queueItem] If there are unfetched queue items left, this will be the oldest one found. If not, this will be `null`.\n */\n\n/**\n * Gets the first unfetched item in the queue\n * @param {FetchQueue~oldestUnfetchedItemCallback} callback\n */\nFetchQueue.prototype.oldestUnfetchedItem = function(callback) {\n    var queue = this;\n\n    for (var i = queue._oldestUnfetchedIndex; i < queue.length; i++) {\n        if (queue[i].status === \"queued\") {\n            queue._oldestUnfetchedIndex = i;\n            callback(null, queue[i]);\n            return;\n        }\n    }\n\n    // When no unfetched queue items remain, we previously called back with an\n    // error, but since it's not really an error condition, we opted to just\n    // call back with (null, null) instead\n    callback(null, null);\n};\n\n/**\n * Called when {@link FetchQueue#max} returns a result\n * @callback FetchQueue~maxCallback\n * @param {Error} [error] If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Number} [max]  The maximum value of the property that was initially provided\n */\n\n/**\n * Gets the maximum value of a stateData property from all the items in the\n * queue. This means you can eg. get the maximum request time, download size\n * etc.\n * @param {String} statisticName            Can be any of the strings in {@link FetchQueue._allowedStatistics}\n * @param {FetchQueue~maxCallback} callback\n */\nFetchQueue.prototype.max = function(statisticName, callback) {\n    var maximum = 0,\n        queue = this;\n\n    if (queue._allowedStatistics.indexOf(statisticName) === -1) {\n        return callback(new Error(\"Invalid statistic\"));\n    }\n\n    queue.forEach(function(item) {\n        if (item.fetched && item.stateData[statisticName] > maximum) {\n            maximum = item.stateData[statisticName];\n        }\n    });\n\n    callback(null, maximum);\n};\n\n/**\n * Called when {@link FetchQueue#min} returns a result\n * @callback FetchQueue~minCallback\n * @param {Error} [error] If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Number} [min]  The minimum value of the property that was initially provided\n */\n\n/**\n * Gets the minimum value of a stateData property from all the items in the\n * queue. This means you can eg. get the minimum request time, download size\n * etc.\n * @param {String} statisticName            Can be any of the strings in {@link FetchQueue._allowedStatistics}\n * @param {FetchQueue~minCallback} callback\n */\nFetchQueue.prototype.min = function(statisticName, callback) {\n    var minimum = Infinity,\n        queue = this;\n\n    if (queue._allowedStatistics.indexOf(statisticName) === -1) {\n        return callback(new Error(\"Invalid statistic\"));\n    }\n\n    queue.forEach(function(item) {\n        if (item.fetched && item.stateData[statisticName] < minimum) {\n            minimum = item.stateData[statisticName];\n        }\n    });\n\n    callback(null, minimum === Infinity ? 0 : minimum);\n};\n\n/**\n * Called when {@link FetchQueue#avg} returns a result\n * @callback FetchQueue~avgCallback\n * @param {Error} [error] If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Number} [avg]  The average value of the property that was initially provided\n */\n\n/**\n * Gets the average value of a stateData property from all the items in the\n * queue. This means you can eg. get the average request time, download size\n * etc.\n * @param {String} statisticName            Can be any of the strings in {@link FetchQueue._allowedStatistics}\n * @param {FetchQueue~avgCallback} callback\n */\nFetchQueue.prototype.avg = function(statisticName, callback) {\n    var sum = 0,\n        count = 0,\n        queue = this;\n\n    if (queue._allowedStatistics.indexOf(statisticName) === -1) {\n        return callback(new Error(\"Invalid statistic\"));\n    }\n\n    queue.forEach(function(item) {\n        if (item.fetched && Number.isFinite(item.stateData[statisticName])) {\n            sum += item.stateData[statisticName];\n            count++;\n        }\n    });\n\n    callback(null, sum / count);\n};\n\n/**\n * Called when {@link FetchQueue#countItems} returns a result\n * @callback FetchQueue~countItemsCallback\n * @param {Error} [error]  If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Number} [count] The number of items that matched the provided selector\n */\n\n/**\n * Counts the items in the queue that match a selector\n * @param {Object} comparator                      Comparator object used to filter items. Queue items that are counted need to match all the properties of this object.\n * @param {FetchQueue~countItemsCallback} callback\n */\nFetchQueue.prototype.countItems = function(comparator, callback) {\n    this.filterItems(comparator, function(error, items) {\n        if (error) {\n            callback(error);\n        } else {\n            callback(null, items.length);\n        }\n    });\n};\n\n/**\n * Called when {@link FetchQueue#filterItems} returns a result\n * @callback FetchQueue~filterItemsCallback\n * @param {Error} [error]       If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {QueueItem[]} [items] The items that matched the provided selector\n */\n\n/**\n * Filters and returns the items in the queue that match a selector\n * @param {Object} comparator                       Comparator object used to filter items. Queue items that are returned need to match all the properties of this object.\n * @param {FetchQueue~filterItemsCallback} callback\n */\nFetchQueue.prototype.filterItems = function(comparator, callback) {\n    var items = this.filter(function(queueItem) {\n        return compare(comparator, queueItem);\n    });\n\n    callback(null, items);\n};\n\n/**\n * Called when {@link FetchQueue#getLength} returns a result\n * @callback FetchQueue~getLengthCallback\n * @param {Error} [error]  If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Number} [items] The total number of items in the queue\n */\n\n/**\n * Gets the total number of queue items in the queue\n * @param {FetchQueue~getLengthCallback} callback\n */\nFetchQueue.prototype.getLength = function(callback) {\n    callback(null, this.length);\n};\n\n/**\n * Called when {@link FetchQueue#freeze} returns a result\n * @callback FetchQueue~freezeCallback\n * @param {Error} [error] If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n */\n\n/**\n * Writes the queue to disk in a JSON file. This file can later be imported\n * using {@link FetchQueue#defrost}\n * @param {String} filename                    Filename passed directly to [fs.writeFile]{@link https://nodejs.org/api/fs.html#fs_fs_writefile_file_data_options_callback}\n * @param {FetchQueue~freezeCallback} callback\n */\nFetchQueue.prototype.freeze = function(filename, callback) {\n    var queue = this;\n\n    // Re-queue in-progress items before freezing...\n    queue.forEach(function(item) {\n        if (item.fetched !== true) {\n            item.status = \"queued\";\n        }\n    });\n\n    fs.writeFile(filename, JSON.stringify(queue, null, 2), function(err) {\n        callback(err);\n    });\n};\n\n/**\n * Called when {@link FetchQueue#defrost} returns a result\n * @callback FetchQueue~defrostCallback\n * @param {Error} [error] If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n */\n\n/**\n * Import the queue from a frozen JSON file on disk.\n * @param {String} filename                     Filename passed directly to [fs.readFile]{@link https://nodejs.org/api/fs.html#fs_fs_readfile_file_options_callback}\n * @param {FetchQueue~defrostCallback} callback\n */\nFetchQueue.prototype.defrost = function(filename, callback) {\n    var queue = this,\n        defrostedQueue = [];\n\n    fs.readFile(filename, function(err, fileData) {\n        if (err) {\n            return callback(err);\n        }\n\n        if (!fileData.toString(\"utf8\").length) {\n            return callback(new Error(\"Failed to defrost queue from zero-length JSON.\"));\n        }\n\n        try {\n            defrostedQueue = JSON.parse(fileData.toString(\"utf8\"));\n        } catch (error) {\n            return callback(error);\n        }\n\n        queue._oldestUnfetchedIndex = defrostedQueue.length - 1;\n        queue._scanIndex = {};\n\n        for (var i = 0; i < defrostedQueue.length; i++) {\n            var queueItem = defrostedQueue[i];\n            queue.push(queueItem);\n\n            if (queueItem.status === \"queued\") {\n                queue._oldestUnfetchedIndex = Math.min(queue._oldestUnfetchedIndex, i);\n            }\n\n            queue._scanIndex[queueItem.url] = true;\n        }\n\n        callback(null, queue);\n    });\n};\n\nmodule.exports = FetchQueue;\n","/home/travis/build/npmtest/node-npmtest-simplecrawler/node_modules/simplecrawler/lib/cookies.js":"/**\n * @file simplecrawler's cookie jar module\n */\n\nvar EventEmitter = require(\"events\").EventEmitter,\n    util = require(\"util\");\n\n/**\n * Creates a new cookie jar\n * @class\n */\nvar CookieJar = function() {\n    EventEmitter.call(this);\n\n    /**\n     * The actual jar that holds the cookies\n     * @private\n     * @type {Array}\n     */\n    this.cookies = [];\n};\n\nutil.inherits(CookieJar, EventEmitter);\n\n/**\n * Called when {@link CookieJar#add} returns a result\n * @callback CookieJar~addCallback\n * @param {Error|null} error   If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Cookie|null} cookie The cookie that was added to the jar\n */\n\n/**\n * Adds a new cookie to the jar, either by creating a new {@link Cookie} object\n * from specific details such as name, value, etc., accepting a string from a\n * Set-Cookie header, or by passing in an existing {@link Cookie} object.\n * @fires CookieJar#addcookie\n * @param {String} name                       Name of the new cookie\n * @param {String} value                      Value of the new cookie\n * @param {String|Number} expiry              Expiry timestamp of the new cookie in milliseconds\n * @param {String} [path=\"/\"]                 Limits cookie to a path\n * @param {String} [domain=\"*\"]               Limits cookie to a domain\n * @param {Boolean} [httponly=false]          Specifies whether to include the HttpOnly flag\n * @param {CookieJar~addCallback} [callback]\n * @return {CookieJar}                        Returns the cookie jar instance to enable chained API calls\n */\nCookieJar.prototype.add = function(name, value, expiry, path, domain, httponly, callback) {\n    var existingIndex = -1, newCookie;\n\n    if (arguments.length > 1) {\n        newCookie = new Cookie(name, value, expiry, path, domain, httponly);\n    } else if (name instanceof Cookie) {\n        newCookie = name;\n    } else {\n        newCookie = Cookie.fromString(name);\n    }\n\n    // Are we updating an existing cookie or adding a new one?\n    this.cookies.forEach(function(cookie, index) {\n        if (cookie.name === newCookie.name && cookie.matchDomain(newCookie.domain)) {\n            existingIndex = index;\n        }\n    });\n\n    if (existingIndex === -1) {\n        this.cookies.push(newCookie);\n    } else {\n        this.cookies[existingIndex] = newCookie;\n    }\n\n    /**\n     * Fired when a cookie has been added to the jar\n     * @event CookieJar#addcookie\n     * @param {Cookie} cookie The cookie that has been added\n     */\n    this.emit(\"addcookie\", newCookie);\n\n    if (callback instanceof Function) {\n        callback(null, newCookie);\n    }\n\n    return this;\n};\n\n/**\n * Called when {@link CookieJar#remove} returns a result\n * @callback CookieJar~removeCallback\n * @param {Error|null} error             If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Cookie[]|null} cookiesRemoved An array of the cookies that were removed from the cookie jar\n */\n\n/**\n * Removes cookies from the cookie jar. If no domain and name are specified, all\n * cookies in the jar are removed.\n * @fires CookieJar#removecookie\n * @param {String} [name]                       Name of the cookie to be removed\n * @param {String} [domain]                     The domain that the cookie applies to\n * @param {CookieJar~removeCallback} [callback]\n * @return {Cookie[]}                           Returns an array of the cookies that were removed from the cookie jar\n */\nCookieJar.prototype.remove = function(name, domain, callback) {\n    var cookiesRemoved = [],\n        jar = this;\n\n    jar.cookies.forEach(function(cookie, index) {\n        // If the names don't match, we're not removing this cookie\n        if (Boolean(name) && cookie.name !== name) {\n            return false;\n        }\n\n        // If the domains don't match, we're not removing this cookie\n        if (Boolean(domain) && !cookie.matchDomain(domain)) {\n            return false;\n        }\n\n        // Matched. Remove!\n        cookiesRemoved.push(jar.cookies.splice(index, 1));\n    });\n\n    /**\n     * Fired when one or multiple cookie have been removed from the jar\n     * @event CookieJar#removecookie\n     * @param {Cookie[]} cookie The cookies that have been removed\n     */\n    jar.emit(\"removecookie\", cookiesRemoved);\n\n    if (callback instanceof Function) {\n        callback(null, cookiesRemoved);\n    }\n\n    return cookiesRemoved;\n};\n\n/**\n * Called when {@link CookieJar#get} returns a result\n * @callback CookieJar~getCallback\n * @param {Error} [error]      If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {Cookie[]} [cookies] An array of cookies that matched the name and/or domain.\n */\n\n/**\n * Gets an array of cookies based on name and domain\n * @param  {String} [name]                    Name of the cookie to retrieve\n * @param  {String} [domain]                  Domain to retrieve the cookies from\n * @param  {CookieJar~getCallback} [callback]\n * @return {Cookie[]}                         Returns an array of cookies that matched the name and/or domain\n */\nCookieJar.prototype.get = function(name, domain, callback) {\n    var cookies = this.cookies.filter(function(cookie) {\n        // If the names don't match, we're not returning this cookie\n        if (Boolean(name) && cookie.name !== name) {\n            return false;\n        }\n\n        // If the domains don't match, we're not returning this cookie\n        if (Boolean(domain) && !cookie.matchDomain(domain)) {\n            return false;\n        }\n\n        return true;\n    });\n\n    if (callback instanceof Function) {\n        callback(null, cookies);\n    }\n\n    return cookies;\n};\n\n/**\n * Called when {@link CookieJar#getAsHeader} returns a result\n * @callback CookieJar~getAsHeaderCallback\n * @param {Error} [error]      If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n * @param {String[]} [cookies] An array of HTTP header formatted cookies.\n */\n\n/**\n * Generates an array of headers based on the value of the cookie jar\n * @param {String} [domain]                          The domain from which to generate cookies\n * @param {String} [path]                            Filter headers to cookies applicable to this path\n * @param {CookieJar~getAsHeaderCallback} [callback]\n * @return {String[]}                                Returns an array of HTTP header formatted cookies\n */\nCookieJar.prototype.getAsHeader = function(domain, path, callback) {\n    var headers = this.cookies.filter(function(cookie) {\n        if (cookie.isExpired()) {\n            return false;\n        }\n        if (!domain && !path) {\n            return true;\n        }\n        if (domain) {\n            return cookie.matchDomain(domain);\n        }\n        if (path) {\n            return cookie.matchPath(path);\n        }\n    })\n    .map(function(cookie) {\n        return cookie.toString();\n    });\n\n    if (callback instanceof Function) {\n        callback(null, headers);\n    }\n\n    return headers;\n};\n\n/**\n * Called when {@link CookieJar#addFromHeaders} returns a result\n * @callback CookieJar~addFromHeadersCallback\n * @param {Error} [error] If the operation was successful, this will be `null`. Otherwise it will be the error that was encountered.\n */\n\n/**\n * Adds cookies to the cookie jar based on an array of 'Set-Cookie' headers\n * provided by a web server. Duplicate cookies are overwritten.\n * @fires CookieJar#addcookie\n * @param {String|String[]} headers                     One or multiple Set-Cookie headers to be added to the cookie jar\n * @param {CookieJar~addFromHeadersCallback} [callback]\n * @return {CookieJar}                                  Returns the cookie jar instance to enable chained API calls\n */\nCookieJar.prototype.addFromHeaders = function(headers, callback) {\n    var jar = this;\n\n    if (!Array.isArray(headers)) {\n        headers = [headers];\n    }\n\n    headers.forEach(function(header) {\n        jar.add(header);\n    });\n\n    if (callback instanceof Function) {\n        callback(null);\n    }\n\n    return jar;\n};\n\n/**\n * Generates a newline-separated list of all cookies in the jar\n * @return {String} Returns stringified versions of all cookies in the jar in a newline separated string\n */\nCookieJar.prototype.toString = function() {\n    return this.getAsHeader().join(\"\\n\");\n};\n\n\n/**\n * Creates a new cookies\n * @class\n * @param {String} name                       Name of the new cookie\n * @param {String} value                      Value of the new cookie\n * @param {String|Number} expires             Expiry timestamp of the new cookie in milliseconds\n * @param {String} [path=\"/\"]                 Limits cookie to a path\n * @param {String} [domain=\"*\"]               Limits cookie to a domain\n * @param {Boolean} [httponly=false]          Specifies whether to include the HttpOnly flag\n */\nvar Cookie = function(name, value, expires, path, domain, httponly) {\n    if (!name) {\n        throw new Error(\"A name is required to create a cookie.\");\n    }\n\n    // Parse date to timestamp - consider it never expiring if timestamp is not\n    // passed to the function\n    if (expires) {\n\n        if (typeof expires !== \"number\") {\n            expires = (new Date(expires)).getTime();\n        }\n\n    } else {\n        expires = -1;\n    }\n\n    this.name = name;\n    this.value = value || \"\";\n    this.expires = expires;\n    this.path = path || \"/\";\n    this.domain = domain || \"*\";\n    this.httponly = Boolean(httponly);\n};\n\n/**\n * Creates a new {@link Cookie} based on a header string\n * @param  {String} string A Set-Cookie header string\n * @return {Cookie}        Returns a newly created Cookie object\n */\nCookie.fromString = function(string) {\n\n    if (!string || typeof string !== \"string\") {\n        throw new Error(\"String must be supplied to generate a cookie.\");\n    }\n\n    function parseKeyVal(input) {\n        var key = input.split(/=/).shift(),\n            val = input.split(/=/).slice(1).join(\"=\");\n\n        return [key, val];\n    }\n\n    string = string.replace(/^\\s*set\\-cookie\\s*:\\s*/i, \"\");\n\n    var parts = string.split(/\\s*;\\s*/i),\n        name = parseKeyVal(parts.shift()),\n        keyValParts = {};\n\n    keyValParts.name = name[0];\n    keyValParts.value = name[1];\n\n    parts\n        .filter(function(input) {\n            return Boolean(input.replace(/\\s+/ig, \"\").length);\n        })\n        .map(parseKeyVal)\n        .forEach(function(keyval) {\n            var key = String(keyval[0]).toLowerCase().replace(/[^a-z0-9]/ig, \"\");\n            keyValParts[key] = keyval[1];\n        });\n\n    return new Cookie(\n        keyValParts.name,\n        keyValParts.value,\n        keyValParts.expires || keyValParts.expiry,\n        keyValParts.path,\n        keyValParts.domain,\n        keyValParts.hasOwnProperty(\"httponly\")\n    );\n};\n\n/**\n * Outputs the cookie as a string, in the form of a Set-Cookie header\n * @param  {Boolean} [includeHeader] Controls whether to include the 'Set-Cookie: ' header name at the beginning of the string.\n * @return {String}                  Stringified version of the cookie\n */\nCookie.prototype.toString = function(includeHeader) {\n    var string = \"\";\n\n    if (includeHeader) {\n        string = \"Set-Cookie: \";\n    }\n\n    string += this.name + \"=\" + this.value + \"; \";\n\n    if (this.expires > 0) {\n        string += \"Expires=\" + (new Date(this.expires)).toGMTString() + \"; \";\n    }\n\n    if (this.path) {\n        string += \"Path=\" + this.path + \"; \";\n    }\n\n    if (this.domain) {\n        string += \"Domain=\" + this.domain + \"; \";\n    }\n\n    if (this.httponly) {\n        string += \"Httponly; \";\n    }\n\n    return string;\n};\n\n/**\n * Determines whether a cookie has expired or not\n * @return {Boolean} Returns true if the cookie has expired. Otherwise, it returns false.\n */\nCookie.prototype.isExpired = function() {\n    if (this.expires < 0) {\n        return false;\n    }\n    return this.expires < Date.now();\n};\n\n/**\n * Determines whether a cookie matches a given domain\n * @param  {String} domain The domain to match against\n * @return {Boolean}       Returns true if the provided domain matches the cookie's domain. Otherwise, it returns false.\n */\nCookie.prototype.matchDomain = function(domain) {\n    if (this.domain === \"*\") {\n        return true;\n    }\n\n    var reverseDomain = this.domain.split(\"\").reverse().join(\"\"),\n        reverseDomainComp = domain.split(\"\").reverse().join(\"\");\n\n    return reverseDomain.indexOf(reverseDomainComp) === 0;\n};\n\n/**\n * Determines whether a cookie matches a given path\n * @param  {String} path The path to match against\n * @return {Boolean}     Returns true if the provided path matches the cookie's path. Otherwise, it returns false.\n */\nCookie.prototype.matchPath = function(path) {\n    if (!this.path) {\n        return true;\n    }\n\n    return path.indexOf(this.path) === 0;\n};\n\nmodule.exports = CookieJar;\nmodule.exports.Cookie = Cookie;\n","/home/travis/build/npmtest/node-npmtest-simplecrawler/node_modules/simplecrawler/lib/cache.js":"/*\n * Simplecrawler - cache module\n * https://github.com/cgiffard/node-simplecrawler\n *\n * Copyright (c) 2011-2015, Christopher Giffard\n *\n */\n\nvar EventEmitter = require(\"events\").EventEmitter;\nvar FilesystemBackend = require(\"./cache-backend-fs.js\");\n\n// Init cache wrapper for backend...\nvar Cache = function Cache(cacheLoadParameter, cacheBackend) {\n\n    // Ensure parameters are how we want them...\n    cacheBackend = typeof cacheBackend === \"object\" ? cacheBackend : FilesystemBackend;\n    cacheLoadParameter = cacheLoadParameter instanceof Array ? cacheLoadParameter : [cacheLoadParameter];\n\n    // Now we can just run the factory.\n    this.datastore = cacheBackend.apply(cacheBackend, cacheLoadParameter);\n\n    // Instruct the backend to load up.\n    this.datastore.load();\n};\n\nCache.prototype = new EventEmitter();\n\n// Set up data import and export functions\nCache.prototype.setCacheData = function(queueObject, data, callback) {\n    this.datastore.setItem(queueObject, data, callback);\n    this.emit(\"setcache\", queueObject, data);\n};\n\nCache.prototype.getCacheData = function(queueObject, callback) {\n    this.datastore.getItem(queueObject, callback);\n};\n\nCache.prototype.saveCache = function() {\n    this.datastore.saveCache();\n};\n\nmodule.exports = Cache;\nmodule.exports.Cache = Cache;\nmodule.exports.FilesystemBackend = FilesystemBackend;\n","/home/travis/build/npmtest/node-npmtest-simplecrawler/node_modules/simplecrawler/lib/cache-backend-fs.js":"/*\n * Simplecrawler - FS cache backend\n * https://github.com/cgiffard/node-simplecrawler\n *\n * Copyright (c) 2011-2015, Christopher Giffard\n *\n */\n\n// Tries to ensure a local 'cache' of a website is as close as possible to a mirror of the website itself.\n// The idea is that it is then possible to re-serve the website just using the cache.\n\nvar fs = require(\"fs\"),\n    crypto = require(\"crypto\");\n\n// Factory for FSBackend\nvar backend = function backend(loadParameter) {\n    return new FSBackend(loadParameter);\n};\n\nmodule.exports = backend;\n\n// Constructor for filesystem cache backend\nvar FSBackend = function FSBackend(loadParameter) {\n    this.loaded = false;\n    this.index = [];\n    this.location = typeof loadParameter === \"string\" && loadParameter.length > 0 ? loadParameter : process.cwd() + \"/cache/\";\n    this.location = this.location.substr(this.location.length - 1) === \"/\" ? this.location : this.location + \"/\";\n};\n\n// Function for sanitising paths\n// We try to get the most understandable, file-system friendly paths we can.\n// An extension is added if not present or inappropriate - if a better one can be determined.\n// Querystrings are hashed to truncate without (hopefully) collision.\n\nfunction sanitisePath(path, queueObject) {\n    // Remove first slash (as we set one later.)\n    path = path.replace(/^\\//, \"\");\n\n    var pathStack = [];\n\n    // Trim whitespace. If no path is present - assume index.html.\n    var sanitisedPath = path.length ? path.replace(/\\s*$/ig, \"\") : \"index.html\";\n    var headers = queueObject.stateData.headers, sanitisedPathParts;\n\n    if (sanitisedPath.match(/\\?/)) {\n        sanitisedPathParts = sanitisedPath.split(/\\?/g);\n        var resource = sanitisedPathParts.shift();\n        var hashedQS = crypto.createHash(\"sha1\").update(sanitisedPathParts.join(\"?\")).digest(\"hex\");\n        sanitisedPath = resource + \"?\" + hashedQS;\n    }\n\n    pathStack = sanitisedPath.split(/\\//g);\n    pathStack = pathStack.map(function(pathChunk) {\n        if (pathChunk.length >= 250) {\n            return crypto.createHash(\"sha1\").update(pathChunk).digest(\"hex\");\n        }\n\n        return pathChunk;\n    });\n\n    sanitisedPath = pathStack.join(\"/\");\n\n    // Try to get a file extension for the file - for ease of identification\n    // We run through this if we either:\n    //  1) haven't got a file extension at all, or:\n    //  2) have an HTML file without an HTML file extension (might be .php, .aspx, .do, or some other server-processed type)\n\n    if (!sanitisedPath.match(/\\.[a-z0-9]{1,6}$/i) || headers[\"content-type\"] && headers[\"content-type\"].match(/text\\/html/i) && !sanitisedPath.match(/\\.htm[l]?$/i)) {\n        var subMimeType = \"\";\n        var mimeParts = [];\n\n        if (headers[\"content-type\"] && headers[\"content-type\"].match(/text\\/html/i)) {\n            if (sanitisedPath.match(/\\/$/)) {\n                sanitisedPath += \"index.html\";\n            } else {\n                sanitisedPath += \".html\";\n            }\n\n        } else if (headers[\"content-type\"] && (mimeParts = headers[\"content-type\"].match(/(image|video|audio|application)\\/([a-z0-9]+)/i))) {\n            subMimeType = mimeParts[2];\n            sanitisedPath += \".\" + subMimeType;\n        }\n    }\n\n    return sanitisedPath;\n}\n\nFSBackend.prototype.fileExists = function(location) {\n    try {\n        fs.statSync(location);\n        return true;\n    } catch (er) {\n        return false;\n    }\n};\n\nFSBackend.prototype.isDirectory = function(location) {\n    try {\n        if (fs.statSync(location).isDirectory()) {\n            return true;\n        }\n\n        return false;\n    } catch (er) {\n        return false;\n    }\n};\n\nFSBackend.prototype.load = function() {\n    var backend = this;\n\n    if (!backend.fileExists(backend.location) && backend.isDirectory(backend.location)) {\n        throw new Error(\"Unable to verify cache location exists.\");\n    }\n\n    try {\n        var fileData;\n        if ((fileData = fs.readFileSync(backend.location + \"cacheindex.json\")) && fileData.length) {\n            backend.index = JSON.parse(fileData.toString(\"utf8\"));\n            backend.loaded = true;\n        }\n    } catch (error) {\n        if (error.code === \"ENOENT\") {\n            // Cache index doesn't exist. Assume this is a new cache.\n            // Just leave the memory index empty for now.\n            backend.loaded = true;\n        } else {\n            throw error;\n        }\n    }\n\n    // Flush store to disk when closing.\n    process.on(\"exit\", function() {\n        backend.saveCache.apply(backend);\n    });\n};\n\nFSBackend.prototype.saveCache = function(callback) {\n    fs.writeFile(this.location + \"cacheindex.json\", JSON.stringify(this.index), callback);\n};\n\nFSBackend.prototype.setItem = function(queueObject, data, callback) {\n    callback = callback instanceof Function ? callback : function() {};\n\n    var backend = this;\n    var pathStack = [queueObject.protocol, queueObject.host, queueObject.port];\n    pathStack = pathStack.concat(sanitisePath(queueObject.path, queueObject).split(/\\/+/g));\n\n    var cacheItemExists = false;\n    var firstInstanceIndex = NaN;\n    if (backend.index.reduce(function(prev, current, index) {\n        firstInstanceIndex = !isNaN(firstInstanceIndex) ? firstInstanceIndex : index;\n        return prev || current.url === queueObject.url;\n    }, false)) {\n        cacheItemExists = true;\n    }\n\n    var writeFileData = function(currentPath, data) {\n        fs.writeFile(currentPath, data, function(error) {\n            if (error) {\n                throw error;\n            }\n            fs.writeFile(currentPath + \".cacheData.json\", JSON.stringify(queueObject), function(error) {\n                if (error) {\n                    throw error;\n                }\n\n                var cacheObject = {\n                    url: queueObject.url,\n                    etag: queueObject.stateData.headers.etag,\n                    lastModified: queueObject.stateData.headers[\"last-modified\"],\n                    dataFile: currentPath,\n                    metaFile: currentPath + \".cacheData.json\"\n                };\n\n                if (cacheItemExists) {\n                    backend.index[firstInstanceIndex] = cacheObject;\n                } else {\n                    backend.index.push(cacheObject);\n                }\n\n                callback(cacheObject);\n            });\n        });\n    };\n\n    pathStack.forEach(function(pathChunk, count) {\n        var currentPath = backend.location + pathStack.slice(0, count + 1).join(\"/\");\n        if (backend.fileExists(backend.location + pathStack.slice(0, count + 1).join(\"/\"))) {\n            if (!backend.isDirectory(currentPath)) {\n                if (count === pathStack.length - 1) {\n                    // Just overwrite the file...\n                    writeFileData(currentPath, data);\n                } else {\n                    throw new Error(\"Cache storage of resource (%s) blocked by file: %s\", queueObject.url, currentPath);\n                }\n            }\n        } else if (count === pathStack.length - 1) {\n            // Write the file data in\n            writeFileData(currentPath, data);\n        } else {\n            fs.mkdirSync(currentPath);\n        }\n    });\n};\n\nFSBackend.prototype.getItem = function(queueObject, callback) {\n    var cacheItemResult = this.index.filter(function(item) {\n        return item.url === queueObject.url;\n    });\n\n    if (cacheItemResult.length) {\n        var cacheItem = cacheItemResult.shift();\n\n        callback({\n            url: cacheItem.url,\n            etag: cacheItem.etag,\n            lastModified: cacheItem.lastModified,\n            getData: function(callback) {\n                fs.readFile(cacheItem.dataFile, function(error, data) {\n                    if (error) {\n                        callback(error);\n                        return false;\n                    }\n\n                    callback(null, data);\n                });\n            },\n            getMetadata: function(callback) {\n                fs.readFile(cacheItem.metaFile, function(error, data) {\n                    if (error) {\n                        callback(error);\n                        return false;\n                    }\n\n                    callback(null, JSON.parse(data.toString(\"utf8\")));\n                });\n            }\n        });\n\n    } else {\n        callback(null);\n    }\n\n    return false;\n};\n"}